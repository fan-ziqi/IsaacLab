# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2025, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 2.1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-06-14 10:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:4
msgid "Exploring the RL problem"
msgstr "探索 RL 问题"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:6
msgid ""
"The command to the Jetbot is a unit vector in specifying the desired drive "
"direction and we must make the agent aware of this somehow so it can adjust "
"its actions accordingly.  There are many possible ways to do this, with the "
"\"zeroth order\" approach to simply change the observation space to include "
"this command. To start, **edit the ``IsaacLabTutorialEnvCfg`` to set the "
"observation space to 9**: the world velocity vector contains the linear and "
"angular velocities of the robot, which is 6 dimensions and if we append the "
"command to this vector, that's 9 dimensions for the observation space in "
"total."
msgstr ""
"Jetbot "
"的命令是一个单位向量，用来指定期望的驾驶方向，我们必须以某种方式使智能体感知到这一点，以便根据情况调整操作。有许多可能的方法可以做到这一点，简单的方法是改变观测空间，将该命令包含在其中。首先，"
" **编辑 ``IsaacLabTutorialEnvCfg`` 将观测空间设置为 9** : 世界速度向量包含机器人的线性和角速度，这是 6 "
"维，如果我们将命令附加到这个向量中，那就是观测空间总共有 9 维。"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:11
msgid ""
"Next, we just need to do that appending when we get the observations.  We "
"also need to calculate our forward vectors for later use. The forward vector"
" for the Jetbot is the x axis, so we apply the ``root_link_quat_w`` to "
"``[1,0,0]`` to get the forward vector in the world frame. Replace the "
"``_get_observations`` method with the following:"
msgstr ""
"接下来，我们只需要在获取观测时进行附加。我们还需要计算以后使用的前进向量。Jetbot 的前进向量是 x 轴，因此我们将 "
"``root_link_quat_w`` 应用于 ``[1,0,0]`` 以获得世界坐标系中的前进向量。将 ``_get_observations`` "
"方法替换为以下内容: "

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:25
msgid ""
"When the robot is behaving as desired, it will be driving at full speed in "
"the direction of the command. If we reward both \"driving forward\" and "
"\"alignment to the command\", then maximizing that combined signal should "
"result in driving to the command... right?"
msgstr ""
"当机器人表现如期望的那样时，它将全速朝着命令的方向行驶。如果我们奖励 \"前进\" 和 \"与命令对齐\" "
"，那么最大化这两者的组合信号应该导致机器人朝着命令前进... 对吗？"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:28
msgid ""
"Let's give it a try! Replace the ``_get_rewards`` method with the following:"
msgstr "让我们试试吧！将 ``_get_rewards`` 方法替换为以下内容: "

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:38
msgid ""
"The ``forward_reward`` is the x component of the linear center of mass "
"velocity of the robot in the body frame. We know that the x direction is the"
" forward direction for the asset, so this should be equivalent to inner "
"product between the forward vector and the linear velocity in the world "
"frame.  The alignment term is the inner product between the forward vector "
"and the command vector: when they are pointing in the same direction this "
"term will be 1, but in the opposite direction it will be -1.  We add them "
"together to get the combined reward and we can finally run training!  Let's "
"see what happens!"
msgstr ""
"``forward_reward`` 是机器人质心线性速度在机体坐标系中的 x 分量。我们知道 x "
"方向是资产的前进方向，因此这应等同于前进向量与世界坐标系中线性速度的内积。对齐项是前进向量与命令向量的内积: 当它们指向同一方向时，这一项将为 "
"1，但指向相反方向时，它将为 -1。将它们相加以获得组合奖励，最后我们可以开始训练了！让我们看看会发生什么！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:-1
msgid "Naive results"
msgstr "天真的结果"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:54
msgid "Surely we can do better!"
msgstr "我们肯定可以做得更好！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:57
msgid "Reward and Observation Tuning"
msgstr "奖励和观测调整"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:59
msgid ""
"When tuning an environment for training, as a rule of thumb, you want to "
"keep the observation space as small as possible.  This is to reduce the "
"number parameters in the model (the literal interpretation of Occam's razor)"
" and thus improve training time. In this case we need to somehow encode our "
"alignment to the command and our forward speed. One way to do this is to "
"exploit the dot and cross products from linear algebra! Replace the contents"
" of ``_get_observations`` with the following:"
msgstr ""
"当为训练调整环境时，一个经验法则是尽可能保持观测空间尽可能小。这是为了减少模型中的参数数量（奥卡姆剃刀的字面解释），从而改善训练时间。在这种情况下，我们需要以某种方式对与命令的对齐和前进速度进行编码。一种方法是利用线性代数中的点积和叉积！用以下内容替换"
" ``_get_observations`` 的内容: "

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:78
msgid ""
"The dot or inner product tells us how aligned two vectors are as a single "
"scalar quantity.  If they are very aligned and pointed in the same "
"direction, then the inner product will be large and positive, but if they "
"are aligned and in opposite directions, it will be large and negative.  If "
"two vectors are perpendicular, the inner product is zero. This means that "
"the inner product between the forward vector and the command vector can tell"
" us how much we are facing towards or away from the command, but not which "
"direction we need to turn to improve alignment."
msgstr ""
"点积或内积告诉我们两个向量对齐程度的单一标量量。如果它们非常对齐并指向相同方向，那么内积将很大且为正，但如果它们对齐且方向相反，则内积将很大且为负。如果两个向量垂直，内积为零。这意味着前进向量和命令向量之间的内积可以告诉我们朝向命令还是背离命令的程度，但不能告诉我们需要向哪个方向转动以改善对齐。"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:83
#, python-brace-format
msgid ""
"The cross product also tells us how aligned two vectors are, but it "
"expresses this relationship as a vector.  The cross product between any two "
"vectors defines an axis that is perpendicular to the plane containing the "
"two argument vectors, where the direction of the result vector along this "
"axis is determined by the chirality (dimension ordering, or handedness) of "
"the coordinate system. In our case, we can exploit the fact that we are "
"operating in 2D to only examine the z component of the result of "
":math:`\\vec{forward} \\times \\vec{command}`. This component will be zero "
"if the vectors are colinear, positive if the command vector is to the left "
"of forward, and negative if it's to the right."
msgstr ""
"叉乘也告诉我们两个向量的对齐程度，不过它将这种关系表示为向量。任意两个向量之间的叉乘定义了一个垂直于包含两个参数向量的平面的轴，其中结果向量沿此轴的方向由坐标系的手征性（维度排序或右手性）确定。在我们的情况中，我们可以利用我们在"
" 2D 中操作的事实，只需要检查 :math:`\\vec{forward} \\times \\vec{command}` 的结果的 z "
"分量。如果向量共线，这一分量将为零，如果命令向量位于前方，则为正，如果命令向量位于前方的右侧，则为负。"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:89
msgid ""
"Finally, the x component of the center of mass linear velocity tells us our "
"forward speed, with positive being forward and negative being backwards. We "
"stack these together \"horizontally\" (along dim 1) to generate the "
"observations for each Jetbot. This alone improves performance!"
msgstr ""
"最后，质心线性速度的 x 分量告诉我们我们的前进速度，正数表示前进，负数表示后退。我们将这些 \"水平\" 叠加在一起（沿 dim 1）以生成每个 "
"Jetbot 的观测。这样单独就能提高性能！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:-1
msgid "Improved results"
msgstr "改进的结果"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:98
msgid ""
"It seems to qualitatively train better, and the Jetbots are somewhat inching"
" forward... Surely we can do better still!"
msgstr "在质量上训练效果更好，Jetbots 有些许向前移动... 我们肯定还可以做得更好！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:100
msgid ""
"Another rule of thumb for training is to reduce and simplify the reward "
"function as much as possible.  Terms in the reward behave similarly to the "
"logical \"OR\" operation.  In our case, we are rewarding driving forward and"
" being aligned to the command by adding them together, so our agent can be "
"reward for driving forward OR being aligned to the command. To force the "
"agent to learn to drive in the direction of the command, we should only "
"reward the agent driving forward AND being aligned. Logical AND suggests "
"multiplication and therefore the following reward function:"
msgstr ""
"训练的另一个经验法则是尽可能减少和简化奖励函数。奖励函数中的术语类似于逻辑的 \"OR\" "
"运算。在我们的情况下，我们通过将它们相加来奖励前进和对齐命令，这样我们的智能体就可以获得同等奖励来前进或对齐命令。为了强迫智能体学会朝着命令的方向前进，我们应该仅奖励智能体前进且对齐。逻辑的"
" AND 意味着乘法，因此下面是奖励函数: "

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:113
msgid ""
"Now we will only get rewarded for driving forward if our alignment reward is"
" non zero.  Let's see what kind of result this produces!"
msgstr "现在我们只有在对齐奖励为非零时才会因前进而获得奖励。让我们看看这会产生怎样的结果！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:-1
msgid "Tuned results"
msgstr "调整后的结果"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:120
msgid ""
"It definitely trains faster, but the Jetbots have learned to drive in "
"reverse if the command is pointed behind them. This may be desirable in our "
"case, but it shows just how dependent the policy behavior is on the reward "
"function.  In this case, there are **degenerate solutions** to our reward "
"function: The reward is maximized for driving forward and aligned to the "
"command, but if the Jetbot drives in reverse, then the forward term is "
"negative, and if its driving in reverse towards the command, then the "
"alignment term is **also negative**, meaning hat the reward is positive! "
"When you design your own environments, you will run into degenerate "
"solutions like this and a significant amount of reward engineering is "
"devoted to suppressing or supporting these behaviors by modifying the reward"
" function."
msgstr ""
"训练速度确实更快了，但如果命令指向它们后面，Jetbots "
"已经学会后退驾驶。在我们的情况中，这可能是令人满意的，但这也展示了政策行为如何高度取决于奖励函数。在这种情况下，我们的奖励函数存在 **退化解** : "
"驾驶前进并与命令对齐时奖励最大化，但如果 Jetbot 在倒车，那么前进项为负数，如果它朝着命令后退，那么对齐项 **也为负数** "
"，这意味着奖励是正数！当您设计自己的环境时，您会遇到像这样的退化解，并且大量奖励工程任务就是通过修改奖励函数来抑制或支持这些行为。"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:127
msgid ""
"Let's say, in our case, we don't want this behavior. In our case, the "
"alignment term has a domain of ``[-1, 1]``, but we would much prefer it to "
"be mapped only to positive values. We don't want to *eliminate* the sign on "
"the alignment term, rather, we would like large negative values to be near "
"zero, so if we are misaligned, we don't get rewarded. The exponential "
"function accomplishes this!"
msgstr ""
"假设在我们的情况下，我们不希望出现这种行为。在我们的情况中，对齐项的定义域是 ``[-1, 1]`` ，但我们更希望它仅映射到正值。我们不希望 *消除*"
" 对齐项上的符号，相反，我们希望大负值接近零，这样当我们不对齐时就不会得到奖励。指数函数可以实现这一点！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:139
msgid ""
"Now when we train, the Jetbots will turn to always drive towards the command"
" in the forward direction!"
msgstr "现在当我们进行训练时，Jetbots 将始终朝着命令以前进方向前进！"

#: ../../source/setup/walkthrough/training_jetbot_reward_exploration.rst:-1
msgid "Directed results"
msgstr "导向的结果"
