# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-01-31 19:21+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/tutorials/03_envs/run_rl_training.rst:4
msgid "Training with an RL Agent"
msgstr "与 RL Agent 进行训练"

#: ../../source/tutorials/03_envs/run_rl_training.rst:8
msgid ""
"In the previous tutorials, we covered how to define an RL task environment, "
"register it into the ``gym`` registry, and interact with it using a random "
"agent. We now move on to the next step: training an RL agent to solve the "
"task."
msgstr ""
"在之前的教程中，我们介绍了如何定义一个 RL 任务环境、将其注册到 ``gym`` 注册表中，并使用一个随机 agent "
"与其交互。现在我们继续进行下一步: 训练一个 RL agent 来解决这个任务。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:12
msgid ""
"Although the :class:`envs.ManagerBasedRLEnv` conforms to the "
":class:`gymnasium.Env` interface, it is not exactly a ``gym`` environment. "
"The input and outputs of the environment are not numpy arrays, but rather "
"based on torch tensors with the first dimension being the number of "
"environment instances."
msgstr ""
"尽管 :class:`envs.ManagerBasedRLEnv` 符合 :class:`gymnasium.Env` 接口，但它并不完全是一个 "
"``gym`` 环境。环境的输入和输出不是 numpy 数组，而是基于torch tensors，其中第一个维度是环境实例的数量。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:17
msgid ""
"Additionally, most RL libraries expect their own variation of an environment"
" interface. For example, `Stable-Baselines3`_ expects the environment to "
"conform to its `VecEnv API`_ which expects a list of numpy arrays instead of"
" a single tensor. Similarly, `RSL-RL`_, `RL-Games`_ and `SKRL`_ expect a "
"different interface. Since there is no one-size-fits-all solution, we do not"
" base the :class:`envs.ManagerBasedRLEnv` on any particular learning "
"library. Instead, we implement wrappers to convert the environment into the "
"expected interface. These are specified in the :mod:`isaaclab_rl` module."
msgstr ""
"此外，大多数 RL 库都期望其自己的环境接口变体。例如， `Stable-Baselines3`_ 期望环境符合其 `VecEnv API`_ ，该 "
"API 期望接收一个 numpy 数组列表而不是一个单一的张量。类似地， `RSL-RL`_ 、`RL-Games`_ 和 `SKRL`_ "
"也预期另一个接口。由于没有一种适合所有情况的解决方案，我们不将 :class:`envs.ManagerBasedRLEnv` "
"基于任何特定的学习库。相反，我们实现了包装器来将环境转换为所期望的接口。这些包装器在 :mod:`isaaclab_rl` 模块中指定。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:25
msgid ""
"In this tutorial, we will use `Stable-Baselines3`_ to train an RL agent to "
"solve the cartpole balancing task."
msgstr "在本教程中，我们将使用 `Stable-Baselines3`_ 来训练一个 RL agent 来解决 cartpole 平衡任务。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:30
msgid ""
"Wrapping the environment with the respective learning framework's wrapper "
"should happen in the end, i.e. after all other wrappers have been applied. "
"This is because the learning framework's wrapper modifies the interpretation"
" of environment's APIs which may no longer be compatible with "
":class:`gymnasium.Env`."
msgstr ""
"在最后，使用所对应的学习框架的包装器对环境进行包装。这是因为学习框架的包装器修改了环境 API 的解释，这可能不再与 "
":class:`gymnasium.Env` 兼容。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:35
msgid "The Code"
msgstr "代码"

#: ../../source/tutorials/03_envs/run_rl_training.rst:37
msgid ""
"For this tutorial, we use the training script from `Stable-Baselines3`_ "
"workflow in the ``scripts/reinforcement_learning/sb3`` directory."
msgstr ""
"在本教程中，我们使用 ``scripts/reinforcement_learning/sb3`` 目录中的 `Stable-Baselines3`_ "
"workflow 的训练脚本。"

#: ../../source/tutorials/03_envs/run_rl_training.rst
msgid "Code for train.py"
msgstr "train.py 代码"

#: ../../source/tutorials/03_envs/run_rl_training.rst:49
msgid "The Code Explained"
msgstr "代码解释"

#: ../../source/tutorials/03_envs/run_rl_training.rst:53
msgid ""
"Most of the code above is boilerplate code to create logging directories, "
"saving the parsed configurations, and setting up different Stable-Baselines3"
" components. For this tutorial, the important part is creating the "
"environment and wrapping it with the Stable-Baselines3 wrapper."
msgstr ""
"上面的大部分代码是创建日志目录、保存解析的配置和设置不同的 Stable-Baselines3 组件的样板代码。对于本教程，重要的部分是创建环境并使用 "
"Stable-Baselines3 包装器对其进行包装。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:57
msgid "There are three wrappers used in the code above:"
msgstr "代码中使用了三个包装器: "

#: ../../source/tutorials/03_envs/run_rl_training.rst:59
msgid ""
":class:`gymnasium.wrappers.RecordVideo`: This wrapper records a video of the"
" environment and saves it to the specified directory. This is useful for "
"visualizing the agent's behavior during training."
msgstr ""
":class:`gymnasium.wrappers.RecordVideo`: 这个包装器记录环境的视频并将其保存到指定目录。这对于在训练过程中可视化"
" agent 的行为非常有用。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:62
msgid ""
":class:`wrappers.sb3.Sb3VecEnvWrapper`: This wrapper converts the "
"environment into a Stable-Baselines3 compatible environment."
msgstr ""
":class:`wrappers.sb3.Sb3VecEnvWrapper`: 这个包装器将环境转换为 Stable-Baselines3 兼容的环境。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:64
msgid ""
"`stable_baselines3.common.vec_env.VecNormalize`_: This wrapper normalizes "
"the environment's observations and rewards."
msgstr ""
"`stable_baselines3.common.vec_env.VecNormalize`_: 这个包装器对环境的观测和奖励进行标准化。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:67
msgid ""
"Each of these wrappers wrap around the previous wrapper by following ``env ="
" wrapper(env, *args, **kwargs)`` repeatedly. The final environment is then "
"used to train the agent. For more information on how these wrappers work, "
"please refer to the :ref:`how-to-env-wrappers` documentation."
msgstr ""
"这些包装器中的每一个都通过反复执行 ``env = wrapper(env, *args, **kwargs)`` "
"来包装前一个包装器。然后使用最终的环境来训练 agent。有关这些包装器如何工作的更多信息，请参考 :ref:`how-to-env-wrappers`"
" 文档。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:72
msgid "The Code Execution"
msgstr "代码执行"

#: ../../source/tutorials/03_envs/run_rl_training.rst:74
msgid ""
"We train a PPO agent from Stable-Baselines3 to solve the cartpole balancing "
"task."
msgstr "我们训练一个从 Stable-Baselines3 学习的 PPO agent 来解决 cartpole 平衡任务。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:77
msgid "Training the agent"
msgstr "训练 agent"

#: ../../source/tutorials/03_envs/run_rl_training.rst:79
msgid ""
"There are three main ways to train the agent. Each of them has their own "
"advantages and disadvantages. It is up to you to decide which one you prefer"
" based on your use case."
msgstr "训练 agent 有三种主要方法。每种方法都有其自己的优点和缺点。根据您的用例，您可以决定使用哪种方法。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:83
msgid "Headless execution"
msgstr "无界面执行"

#: ../../source/tutorials/03_envs/run_rl_training.rst:85
msgid ""
"If the ``--headless`` flag is set, the simulation is not rendered during "
"training. This is useful when training on a remote server or when you do not"
" want to see the simulation. Typically, it speeds up the training process "
"since only physics simulation step is performed."
msgstr ""
"如果设置了 ``--headless`` "
"标志，则在训练过程中不会呈现仿真。当在远程服务器上进行训练或者不想看到仿真时，这很有用。通常情况下，此操作会加快训练过程，因为只执行物理仿真步骤。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:95
msgid "Headless execution with off-screen render"
msgstr "无界面执行与离屏渲染"

#: ../../source/tutorials/03_envs/run_rl_training.rst:97
msgid ""
"Since the above command does not render the simulation, it is not possible "
"to visualize the agent's behavior during training. To visualize the agent's "
"behavior, we pass the ``--enable_cameras`` which enables off-screen "
"rendering. Additionally, we pass the flag ``--video`` which records a video "
"of the agent's behavior during training."
msgstr ""
"由于上述命令不会呈现仿真，所以无法在训练过程中看到 agent 的行为。要可视化 agent 的行为，我们传递 ``--enable_cameras``"
" ，这会启用离屏渲染。此外，我们传递标志 ``--video`` ，这会记录 agent 在训练期间的行为视频。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:106
msgid ""
"The videos are saved to the ``logs/sb3/Isaac-Cartpole-v0/<run-"
"dir>/videos/train`` directory. You can open these videos using any video "
"player."
msgstr ""
"视频保存在 ``logs/sb3/Isaac-Cartpole-v0/<run-dir>/videos/train`` "
"目录中。您可以使用任何视频播放器打开这些视频。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:110
msgid "Interactive execution"
msgstr "交互式执行"

#: ../../source/tutorials/03_envs/run_rl_training.rst:114
msgid ""
"While the above two methods are useful for training the agent, they don't "
"allow you to interact with the simulation to see what is happening. In this "
"case, you can ignore the ``--headless`` flag and run the training script as "
"follows:"
msgstr ""
"虽然上述两种方法对于训练 agent 很有用，但不能让您与仿真进行交互以查看发生了什么。在这种情况下，您可以忽略 ``--headless`` "
"标志并按如下方式运行训练脚本: "

#: ../../source/tutorials/03_envs/run_rl_training.rst:122
msgid ""
"This will open the Isaac Sim window and you can see the agent training in "
"the environment. However, this will slow down the training process since the"
" simulation is rendered on the screen. As a workaround, you can switch "
"between different render modes in the ``\"Isaac Lab\"`` window that is "
"docked on the bottom-right corner of the screen. To learn more about these "
"render modes, please check the :class:`sim.SimulationContext.RenderMode` "
"class."
msgstr ""
"这将打开 Isaac Sim 窗口，您可以看到 agent "
"在环境中进行训练。然而，这会减慢训练过程，因为仿真会在屏幕上呈现。作为变通方法，您可以在屏幕右下角停靠的 ``\"Isaac Lab\"`` "
"窗口中在不同的渲染模式之间切换。要了解更多有关这些渲染模式的信息，请查看 "
":class:`sim.SimulationContext.RenderMode` 类。"

#: ../../source/tutorials/03_envs/run_rl_training.rst:129
msgid "Viewing the logs"
msgstr "查看日志"

#: ../../source/tutorials/03_envs/run_rl_training.rst:131
msgid ""
"On a separate terminal, you can monitor the training progress by executing "
"the following command:"
msgstr "在单独的终端中，您可以通过执行以下命令监视训练进度: "

#: ../../source/tutorials/03_envs/run_rl_training.rst:139
msgid "Playing the trained agent"
msgstr "播放经过训练的 agent"

#: ../../source/tutorials/03_envs/run_rl_training.rst:141
msgid ""
"Once the training is complete, you can visualize the trained agent by "
"executing the following command:"
msgstr "一旦训练完成，您可以通过执行以下命令来可视化经过训练的 agent: "

#: ../../source/tutorials/03_envs/run_rl_training.rst:148
msgid ""
"The above command will load the latest checkpoint from the ``logs/sb3/Isaac-"
"Cartpole-v0`` directory. You can also specify a specific checkpoint by "
"passing the ``--checkpoint`` flag."
msgstr ""
"上述命令将从 ``logs/sb3/Isaac-Cartpole-v0`` 目录加载最新的检查点。您也可以通过传递 ``--checkpoint`` "
"标志指定特定的检查点。"
