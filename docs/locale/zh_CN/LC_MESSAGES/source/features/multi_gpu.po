# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-10 09:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/features/multi_gpu.rst:2
msgid "Multi-GPU and Multi-Node Training"
msgstr "多GPU和多节点训练"

#: ../../source/features/multi_gpu.rst:6
msgid ""
"Isaac Lab supports multi-GPU and multi-node reinforcement learning. "
"Currently, this feature is only available for RL-Games, RSL-RL and skrl "
"libraries workflows. We are working on extending this feature to other "
"workflows."
msgstr ""
"Isaac Lab支持多GPU和多节点的强化学习。目前，此功能仅适用于RL-Games, RSL-"
"RL和skrl库工作流程。我们正在努力将此功能扩展到其他工作流程中。"

#: ../../source/features/multi_gpu.rst:12
msgid ""
"Multi-GPU and multi-node training is only supported on Linux. Windows "
"support is not available at this time. This is due to limitations of the "
"NCCL library on Windows."
msgstr "多GPU和多节点训练仅在Linux上受支持。目前不支持Windows。这是由于Windows上NCCL库的限制。"

#: ../../source/features/multi_gpu.rst:17
msgid "Multi-GPU Training"
msgstr "多GPU训练"

#: ../../source/features/multi_gpu.rst:19
msgid "Isaac Lab supports the following multi-GPU training frameworks:"
msgstr "Isaac Lab 支持以下多GPU训练框架: "

#: ../../source/features/multi_gpu.rst:21
msgid ""
"`Torchrun <https://docs.pytorch.org/docs/stable/elastic/run.html>`_ through "
"`PyTorch distributed <https://pytorch.org/docs/stable/distributed.html>`_"
msgstr ""
"`Torchrun <https://docs.pytorch.org/docs/stable/elastic/run.html>`_ 通过 "
"`PyTorch distributed <https://pytorch.org/docs/stable/distributed.html>`_"

#: ../../source/features/multi_gpu.rst:22
msgid ""
"`JAX distributed "
"<https://jax.readthedocs.io/en/latest/jax.distributed.html>`_"
msgstr ""
"`JAX distributed "
"<https://jax.readthedocs.io/en/latest/jax.distributed.html>`_"

#: ../../source/features/multi_gpu.rst:25
msgid "Pytorch Torchrun Implementation"
msgstr "Pytorch Torchrun 实现"

#: ../../source/features/multi_gpu.rst:27
msgid ""
"We are using `Pytorch Torchrun "
"<https://docs.pytorch.org/docs/stable/elastic/run.html>`_ to manage multi-"
"GPU training. Torchrun manages the distributed training by:"
msgstr ""
"我们正在使用 `PyTorch Torchrun "
"<https://docs.pytorch.org/docs/stable/elastic/run.html>`_ 来管理多GPU训练。Torchrun"
" 通过以下方式管理分布式训练: "

#: ../../source/features/multi_gpu.rst:30
msgid ""
"**Process Management**: Launching one process per GPU, where each process is"
" assigned to a specific GPU."
msgstr "**进程管理**: 为每个GPU启动一个进程，其中每个进程分配给特定的GPU。"

#: ../../source/features/multi_gpu.rst:31
msgid ""
"**Script Execution**: Running the same training script (e.g., RL Games "
"trainer) on each process."
msgstr "**脚本执行**: 在每个进程上运行相同的训练脚本（例如，RL Games 训练器）。"

#: ../../source/features/multi_gpu.rst:32
msgid ""
"**Environment Instances**: Each process creates its own instance of the "
"Isaac Lab environment."
msgstr "**环境实例**: 每个进程都会创建自己的 Isaac Lab 环境实例。"

#: ../../source/features/multi_gpu.rst:33
msgid ""
"**Gradient Synchronization**: Aggregating gradients across all processes and"
" broadcasting the synchronized gradients back to each process after each "
"training step."
msgstr "**梯度同步**: 在每个训练步骤后聚合所有进程的梯度，并将同步的梯度广播回每个进程。"

#: ../../source/features/multi_gpu.rst:37
msgid ""
"Check out this `3 minute youtube video from PyTorch "
"<https://www.youtube.com/watch?v=Cvdhwx-"
"OBBo&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj&index=2>`_ to understand how "
"Torchrun works."
msgstr ""
"请查看这个 `PyTorch 的 3 分钟 YouTube 视频 <https://www.youtube.com/watch?v=Cvdhwx-"
"OBBo&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj&index=2>`_ ，了解 Torchrun 的工作原理。"

#: ../../source/features/multi_gpu.rst:40
msgid "The key components in this setup are:"
msgstr "这个设置中的关键组件是: "

#: ../../source/features/multi_gpu.rst:42
msgid ""
"**Torchrun**: Handles process spawning, communication, and gradient "
"synchronization."
msgstr "**Torchrun**: 处理进程生成、通信和梯度同步。"

#: ../../source/features/multi_gpu.rst:43
msgid ""
"**RL Library**: The reinforcement learning library that runs the actual "
"training algorithm."
msgstr "**RL 库**: 运行实际训练算法的强化学习库。"

#: ../../source/features/multi_gpu.rst:44
msgid ""
"**Isaac Lab**: Provides the simulation environment that each process "
"instantiates independently."
msgstr "**Isaac Lab**: 提供每个过程独立实例化的仿真环境。"

#: ../../source/features/multi_gpu.rst:46
msgid ""
"Under the hood, Torchrun uses the `DistributedDataParallel "
"<https://docs.pytorch.org/docs/2.7/notes/ddp.html#internal-design>`_ module "
"to manage the distributed training. When training with multiple GPUs using "
"Torchrun, the following happens:"
msgstr ""
"在幕后，Torchrun 使用 `DistributedDataParallel "
"<https://docs.pytorch.org/docs/2.7/notes/ddp.html#internal-design>`_ "
"模块来管理分布式训练。使用 Torchrun 在多个 GPU 上训练时，会出现以下情况: "

#: ../../source/features/multi_gpu.rst:49
msgid "Each GPU runs an independent process"
msgstr "每个 GPU 运行独立的进程。"

#: ../../source/features/multi_gpu.rst:50
msgid "Each process executes the full training script"
msgstr "每个进程执行完整训练脚本"

#: ../../source/features/multi_gpu.rst:51
msgid "Each process maintains its own:"
msgstr "每个进程都保持自己的: "

#: ../../source/features/multi_gpu.rst:53
msgid "Isaac Lab environment instance (with *n* parallel environments)"
msgstr "Isaac Lab 环境实例（具有 *n* 个并行环境）"

#: ../../source/features/multi_gpu.rst:54
msgid "Policy network copy"
msgstr "策略网络复制"

#: ../../source/features/multi_gpu.rst:55
msgid "Experience buffer for rollout collection"
msgstr "用于rollout收集的经验缓冲区"

#: ../../source/features/multi_gpu.rst:57
msgid "All processes synchronize only for gradient updates"
msgstr "所有进程仅在梯度更新时进行同步"

#: ../../source/features/multi_gpu.rst:59
msgid ""
"For a deeper dive into how Torchrun works, checkout `PyTorch Docs: "
"DistributedDataParallel - Internal Design "
"<https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`_."
msgstr ""
"要深入了解 Torchrun 的工作原理，请查看 `PyTorch Docs: DistributedDataParallel - Internal "
"Design <https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`_ 。"

#: ../../source/features/multi_gpu.rst:63
msgid "Jax Implementation"
msgstr "Jax 实现"

#: ../../source/features/multi_gpu.rst:66
msgid "JAX is only supported with the skrl library."
msgstr "JAX 仅支持 skrl 库。"

#: ../../source/features/multi_gpu.rst:68
msgid ""
"With JAX, we are using `skrl.utils.distributed.jax "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ Since "
"the ML framework doesn't automatically start multiple processes from a "
"single program invocation, the skrl library provides a module to start them."
msgstr ""
"使用 JAX，我们正在使用 `skrl.utils.distributed.jax "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ "
"。由于机器学习框架不会自动从单个程序调用启动多个进程，所以 skrl 库提供了一个模块来启动它们。"

#: ../../source/features/multi_gpu.rst:-1
msgid "Multi-GPU training paradigm"
msgstr "多GPU训练范式"

#: ../../source/features/multi_gpu.rst:87
msgid "Running Multi-GPU Training"
msgstr "运行多GPU训练"

#: ../../source/features/multi_gpu.rst:89
msgid ""
"To train with multiple GPUs, use the following command, where "
"``--nproc_per_node`` represents the number of available GPUs:"
msgstr "要使用多个GPU进行训练，请使用以下命令，其中 ``--nproc_per_node`` 表示可用的GPU数量: "

#: ../../source/features/multi_gpu.rst
msgid "rl_games"
msgstr "rl_games"

#: ../../source/features/multi_gpu.rst
msgid "rsl_rl"
msgstr "rsl_rl"

#: ../../source/features/multi_gpu.rst
msgid "skrl"
msgstr "skrl"

#: ../../source/features/multi_gpu.rst
msgid "PyTorch"
msgstr "PyTorch"

#: ../../source/features/multi_gpu.rst
msgid "JAX"
msgstr "JAX"

#: ../../source/features/multi_gpu.rst:128
msgid "Multi-Node Training"
msgstr "多节点训练"

#: ../../source/features/multi_gpu.rst:130
msgid ""
"To scale up training beyond multiple GPUs on a single machine, it is also "
"possible to train across multiple nodes. To train across multiple "
"nodes/machines, it is required to launch an individual process on each node."
msgstr "要在单台计算机上跨多个GPU扩展训练，还可以在多个节点上训练。要在多个节点/机器上训练，需要在每个节点上启动一个单独的进程。"

#: ../../source/features/multi_gpu.rst:133
msgid ""
"For the master node, use the following command, where ``--nproc_per_node`` "
"represents the number of available GPUs, and ``--nnodes`` represents the "
"number of nodes:"
msgstr ""
"对于主节点，请使用以下命令，其中 ``--nproc_per_node`` 表示可用的GPU数量， ``--nnodes`` 表示节点的数量: "

#: ../../source/features/multi_gpu.rst:172
msgid ""
"Note that the port (``5555``) can be replaced with any other available port."
msgstr "注意，端口（ ``5555`` ）可以更换为任何其他可用端口。"

#: ../../source/features/multi_gpu.rst:174
msgid ""
"For non-master nodes, use the following command, replacing ``--node_rank`` "
"with the index of each machine:"
msgstr "对于非主节点，请使用以下命令，将 ``--node_rank`` 替换为每台机器的索引: "

#: ../../source/features/multi_gpu.rst:212
msgid ""
"For more details on multi-node training with PyTorch, please visit the "
"`PyTorch documentation "
"<https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html>`_. "
"For more details on multi-node training with JAX, please visit the `skrl "
"documentation "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ and the"
" `JAX documentation "
"<https://jax.readthedocs.io/en/latest/multi_process.html>`_."
msgstr ""
"有关使用PyTorch进行多节点训练的更多详细信息，请访问 `PyTorch 文档 "
"<https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html>`_ "
"。有关使用JAX进行多节点训练的更多详细信息，请访问 `skrl 文档 "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ 和 `JAX "
"文档 <https://jax.readthedocs.io/en/latest/multi_process.html>`_ 。"

#: ../../source/features/multi_gpu.rst:220
msgid ""
"As mentioned in the PyTorch documentation, \"multi-node training is "
"bottlenecked by inter-node communication latencies\". When this latency is "
"high, it is possible multi-node training will perform worse than running on "
"a single node instance."
msgstr "如PyTorch文档中所述， ``多节点训练受到节点间通信延迟的瓶颈`` 。当这种延迟较高时，多节点训练可能表现不如在单节点实例上运行。"
