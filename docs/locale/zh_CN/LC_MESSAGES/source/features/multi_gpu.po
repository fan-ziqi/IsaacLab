# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-12-05 17:45+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/features/multi_gpu.rst:2
msgid "Multi-GPU and Multi-Node Training"
msgstr "多GPU和多节点训练"

#: ../../source/features/multi_gpu.rst:6
msgid ""
"Isaac Lab supports multi-GPU and multi-node reinforcement learning. "
"Currently, this feature is only available for RL-Games and skrl libraries "
"workflows. We are working on extending this feature to other workflows."
msgstr ""
"Isaac Lab支持多GPU和多节点的强化学习。目前，此功能仅适用于RL-Games和skrl库工作流程。我们正在努力将此功能扩展到其他工作流程中。"

#: ../../source/features/multi_gpu.rst:12
msgid ""
"Multi-GPU and multi-node training is only supported on Linux. Windows "
"support is not available at this time. This is due to limitations of the "
"NCCL library on Windows."
msgstr "多GPU和多节点训练仅在Linux上受支持。目前不支持Windows。这是由于Windows上NCCL库的限制。"

#: ../../source/features/multi_gpu.rst:17
msgid "Multi-GPU Training"
msgstr "多GPU训练"

#: ../../source/features/multi_gpu.rst:19
msgid ""
"For complex reinforcement learning environments, it may be desirable to "
"scale up training across multiple GPUs. This is possible in Isaac Lab "
"through the use of the `PyTorch distributed "
"<https://pytorch.org/docs/stable/distributed.html>`_ framework or the `JAX "
"distributed <https://jax.readthedocs.io/en/latest/jax.distributed.html>`_ "
"module respectively."
msgstr ""
"对于复杂的强化学习环境，可能希望跨多个GPU扩展训练。在Isaac Lab中可以通过分别使用 `PyTorch分布式 "
"<https://pytorch.org/docs/stable/distributed.html>`_ 框架或者 `JAX distributed "
"<https://jax.readthedocs.io/en/latest/jax.distributed.html>`_ 模块来实现这一点。"

#: ../../source/features/multi_gpu.rst:24
msgid ""
"In PyTorch, the :meth:`torch.distributed` API is used to launch multiple "
"processes of training, where the number of processes must be equal to or "
"less than the number of GPUs available. Each process runs on a dedicated GPU"
" and launches its own instance of Isaac Sim and the Isaac Lab environment. "
"Each process collects its own rollouts during the training process and has "
"its own copy of the policy network. During training, gradients are "
"aggregated across the processes and broadcasted back to the process at the "
"end of the epoch."
msgstr ""
":meth:`torch.distributed` "
"在PyTorch中，API用于启动多个训练进程，其中进程的数量必须等于或小于可用的GPU数量。每个进程在专用GPU上运行，并启动其自己的Isaac "
"Sim和Isaac Lab环境实例。在训练过程中，梯度在进程之间汇总，并在周期结束时广播回进程。"

#: ../../source/features/multi_gpu.rst:31
msgid ""
"In JAX, since the ML framework doesn't automatically start multiple "
"processes from a single program invocation, the skrl library provides a "
"module to start them."
msgstr "在JAX中，由于机器学习框架不会自动从单个程序调用中启动多个进程，skrl库提供了一个模块来启动它们。"

#: ../../source/features/multi_gpu.rst:-1
msgid "多GPU训练范式"
msgstr "多GPU训练范式"

#: ../../source/features/multi_gpu.rst:48
msgid ""
"To train with multiple GPUs, use the following command, where "
"``--proc_per_node`` represents the number of available GPUs:"
msgstr "要使用多个GPU进行训练，请使用以下命令，其中 ``--proc_per_node`` 表示可用的GPU数量: "

#: ../../source/features/multi_gpu.rst
msgid "rl_games"
msgstr "rl_games"

#: ../../source/features/multi_gpu.rst
msgid "skrl"
msgstr "skrl"

#: ../../source/features/multi_gpu.rst
msgid "PyTorch"
msgstr "PyTorch"

#: ../../source/features/multi_gpu.rst
msgid "JAX"
msgstr "JAX"

#: ../../source/features/multi_gpu.rst:80
msgid "Multi-Node Training"
msgstr "多节点训练"

#: ../../source/features/multi_gpu.rst:82
msgid ""
"To scale up training beyond multiple GPUs on a single machine, it is also "
"possible to train across multiple nodes. To train across multiple "
"nodes/machines, it is required to launch an individual process on each node."
msgstr "要在单台计算机上跨多个GPU扩展训练，还可以在多个节点上训练。要在多个节点/机器上训练，需要在每个节点上启动一个单独的进程。"

#: ../../source/features/multi_gpu.rst:85
msgid ""
"For the master node, use the following command, where ``--nproc_per_node`` "
"represents the number of available GPUs, and ``--nnodes`` represents the "
"number of nodes:"
msgstr ""
"对于主节点，请使用以下命令，其中 ``--nproc_per_node`` 表示可用的GPU数量， ``--nnodes`` 表示节点的数量: "

#: ../../source/features/multi_gpu.rst:117
msgid ""
"Note that the port (``5555``) can be replaced with any other available port."
msgstr "注意，端口（ ``5555`` ）可以更换为任何其他可用端口。"

#: ../../source/features/multi_gpu.rst:119
msgid ""
"For non-master nodes, use the following command, replacing ``--node_rank`` "
"with the index of each machine:"
msgstr "对于非主节点，请使用以下命令，将 ``--node_rank`` 替换为每台机器的索引: "

#: ../../source/features/multi_gpu.rst:150
msgid ""
"For more details on multi-node training with PyTorch, please visit the "
"`PyTorch documentation "
"<https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html>`_. "
"For more details on multi-node training with JAX, please visit the `skrl "
"documentation "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ and the"
" `JAX documentation "
"<https://jax.readthedocs.io/en/latest/multi_process.html>`_."
msgstr ""
"有关使用PyTorch进行多节点训练的更多详细信息，请访问 `PyTorch 文档 "
"<https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html>`_ "
"。有关使用JAX进行多节点训练的更多详细信息，请访问 `skrl 文档 "
"<https://skrl.readthedocs.io/en/latest/api/utils/distributed.html>`_ 和 `JAX "
"文档 <https://jax.readthedocs.io/en/latest/multi_process.html>`_ 。"

#: ../../source/features/multi_gpu.rst:158
msgid ""
"As mentioned in the PyTorch documentation, \"multi-node training is "
"bottlenecked by inter-node communication latencies\". When this latency is "
"high, it is possible multi-node training will perform worse than running on "
"a single node instance."
msgstr "如PyTorch文档中所述， ``多节点训练受到节点间通信延迟的瓶颈`` 。当这种延迟较高时，多节点训练可能表现不如在单节点实例上运行。"
