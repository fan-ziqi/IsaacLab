# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 2.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-01-31 19:21+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/overview/reinforcement-learning/training_guide.rst:2
msgid "Debugging and Training Guide"
msgstr "调试和训练指南"

#: ../../source/overview/reinforcement-learning/training_guide.rst:4
msgid ""
"In this tutorial, we'll guide developers working with Isaac Lab to "
"understand the impact of various parameters on training time, GPU "
"utilization, and memory usage. This is especially helpful for addressing Out"
" of Memory (OOM) errors that commonly occur during reinforcement learning "
"(RL) training. We will touch on common errors seen during RL training in "
"Isaac Lab and provide some guidance on troubleshooting steps."
msgstr ""
"在本教程中，我们将指导使用 Isaac Lab 的开发人员了解各种参数对训练时间、GPU "
"使用率和内存使用的影响。这对于解决在强化学习（RL）训练过程中常见的内存不足（OOM）错误特别有帮助。我们将涉及在 Isaac Lab "
"中强化学习训练期间常见的错误，并提供一些故障排除的指导步骤。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:12
msgid "Training with Parallel Environments"
msgstr "训练与并行环境"

#: ../../source/overview/reinforcement-learning/training_guide.rst:14
msgid ""
"The key RL paradigm of Isaac Lab is to train with many environments in "
"parallel. Here, we define an environment as an instance of a robot or "
"multiple robots interacting with other robots or objects in simulation. By "
"creating multiple environments in parallel, we generate multiple copies of "
"the environment such that the robots in each environment can explore the "
"world independently of other environments. The number of environments thus "
"becomes an important hyperparameter for training. In general, the more "
"environments we have running in parallel, the more data we can collect "
"during rollout, which in turn, provides more data for RL training and allows"
" for faster training since the RL agent can learn from parallel experiences."
msgstr ""
"Isaac Lab 的关键 RL "
"范式是通过并行训练多个环境。在这里，我们将环境定义为机器人或多个机器人与其他机器人或物体在仿真中互动的实例。通过并行创建多个环境，我们生成多个环境副本，使得每个环境中的机器人可以独立于其他环境探索世界。因此，环境的数量成为训练中的一个重要超参数。通常，环境数量越多，训练过程中可以收集的数据就越多，这反过来为"
" RL 训练提供了更多数据，并允许更快速的训练，因为 RL 代理可以从并行的经验中学习。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:22
msgid ""
"However, the number of environments can also be bounded by other factors. "
"Memory can often be a hard constraint on the number of environments we can "
"run in parallel. When more environments are added to the world, the "
"simulation also requires more memory to represent and simulate each object "
"in the scene. The number of environments we can simulate in parallel thus "
"often depend on the amount of memory resources available on the machine. In "
"addition, different forms of simulation can also consume various amounts of "
"memory. For example, objects with high fidelity visual and collision meshes "
"will consume more memory than simple primitive shapes. Deformable simulation"
" will also likely require more memory to simulate than rigid bodies."
msgstr ""
"然而，环境的数量也可能受到其他因素的限制。内存通常是我们可以并行运行的环境数量的硬性约束。当更多的环境被添加到世界中时，仿真还需要更多的内存来表示和仿真场景中的每个物体。因此，我们可以并行仿真的环境数量通常取决于机器上可用的内存资源。此外，不同形式的仿真也可能消耗不同数量的内存。例如，具有高保真度视觉和碰撞网格的物体将比简单的原始形状消耗更多的内存。可变形仿真也可能需要比刚体仿真更多的内存。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:30
msgid ""
"Training with rendering often consumes much higher memory than running with "
"only physics simulation. This is especially true when rendering at "
"relatively large resolutions. Additionally, when training RL policies with "
"image observations, we often also require more memory to hold the rollout "
"trajectories of image buffers and larger networks for the policies. Both of "
"these components will also impact the amount of memory available for the "
"simulation."
msgstr ""
"使用渲染进行训练通常会消耗比仅运行物理模拟时更高的内存。这在以相对较大分辨率进行渲染时尤其如此。此外，在使用图像观察进行强化学习（RL）策略训练时，我们通常还需要更多的内存来存储图像缓冲区的回放轨迹以及用于策略的更大网络。这两个组件还会影响可用于模拟的内存量。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:32
msgid ""
"To reduce memory consumption, one method is to simplify collision meshes of "
"the assets where possible to keep only bare minimum collision shapes "
"required for correct simulation of contacts. Additionally, we recommend only"
" running with the viewport when debugging with a small number of "
"environments. When training with larger number of environments in parallel, "
"it is recommended to run in headless mode to avoid any rendering overhead. "
"If the RL pipeline requires rendering in the loop, make sure to reduce the "
"number of environments, taking into consideration for the dimensions of the "
"image buffers and the size of the policy networks. When hitting out of "
"memory errors, the simplest solution may be to reduce the number of "
"environments."
msgstr ""
"为了减少内存消耗，一种方法是简化资源的碰撞网格，尽可能保留正确模拟接触所需的最基本碰撞形状。此外，我们建议在调试时仅使用视口，并且环境数量较少时进行调试。当以较大数量的环境并行训练时，建议在无头模式下运行，以避免任何渲染开销。如果"
"RL流程需要在循环中进行渲染，确保减少环境数量，并考虑图像缓冲区的维度和策略网络的大小。当遇到内存溢出错误时，最简单的解决方案可能是减少环境数量。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:39
msgid "Hyperparameter Tuning"
msgstr "超参数调优"

#: ../../source/overview/reinforcement-learning/training_guide.rst:41
msgid ""
"Although in many cases, simulating more environments in parallel can yield "
"faster training and better results, there are also cases where diminishing "
"returns are observed when the number of environments reaches certain "
"thresholds. This threshold will vary depending on the complexity of the "
"environment, task, policy setup, and RL algorithm. When more environments "
"are simulated in parallel, each simulation step requires more time to "
"simulate, which will impact the overall training time. When the number of "
"environments is small, this increase in per-step simulation time is often "
"insignificant compared to the increase in training performance from more "
"experiences collected. However, when the number of environments reaches a "
"point, the benefits from having even more experiences for the RL algorithm "
"may start to saturate, and the amount of increased simulation time can "
"outweigh the benefits in training performance."
msgstr ""
"虽然在许多情况下，平行模拟更多环境可以带来更快的训练和更好的结果，但也有一些情况，当环境数量达到某个阈值时，会观察到收益递减的现象。这个阈值会根据环境、任务、策略设置和"
"RL算法的复杂性有所不同。当更多环境被平行模拟时，每个模拟步骤需要更多时间来模拟，这将影响整体训练时间。当环境数量较少时，单步模拟时间的增加通常相较于更多经验收集带来的训练性能提升来说是微不足道的。然而，当环境数量达到某个临界点时，更多经验对"
"RL算法的好处可能开始饱和，增加的模拟时间可能会超过训练性能的提升。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:47
msgid ""
"In contrast to diminishing returns on number of environments that are too "
"large, training with low number of environments can also be challenging. "
"This is often due to the RL policies not getting enough experiences to learn"
" from. To address this issue, it may be helpful to increase the batch size "
"or the horizon length to accommodate for the smaller amount of data "
"collected from lower number of parallel environments. When the number of "
"environments is constrained by available resources, running with parallel "
"GPUs or training across multiple nodes can also help alleviate issues due to"
" limited rollouts."
msgstr ""
"与环境数量过大导致的收益递减相反，使用较少数量的环境进行训练也可能具有挑战性。这通常是由于RL"
"策略没有获得足够的经验供其学习。为了解决这个问题，增加批量大小或时间步长可能会有所帮助，以适应从较少数量的并行环境中收集的较少数据。当环境数量受到可用资源的限制时，使用并行"
"GPU或在多个节点上进行训练也有助于缓解由于有限的回合数而导致的问题。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:54
msgid "Debugging NaNs during Training"
msgstr "调试训练中的 NaNs"

#: ../../source/overview/reinforcement-learning/training_guide.rst:56
msgid ""
"One common error seen during RL training is the appearance of NaNs in the "
"observation buffers, which often get propagated into the policy networks and"
" cause crashes in the downstream training pipeline. In most cases, the "
"appearance of NaNs occur when the simulation becomes unstable. This could be"
" due to drastic actions being applied to the robots that exceed the limits "
"of the simulation, or resets of the assets into invalid states. Some helpful"
" tips to reduce the occurrence of NaNs include proper tuning of the physics "
"parameters for the assets to ensure that joint, velocity, and force limits "
"are within reasonable ranges and the gains are correctly tuned for the "
"robot. It is also a good idea to check that actions applied to the robots "
"are reasonable and will not impose large forces or impulses on the objects. "
"Reducing the timestep of the physics simulation can also help improve "
"accuracy and stability of the simulation, in addition to increasing the "
"solver iterations."
msgstr ""
"在 RL 训练过程中常见的一个错误是观察缓冲区中出现 NaN（非数值），这些 NaN "
"通常会被传播到策略网络中，并导致下游训练流程崩溃。在大多数情况下，NaN "
"的出现是由于仿真不稳定。这可能是由于对机器人施加了超出仿真限制的剧烈动作，或者资产重置到无效状态所导致的。减少 NaN "
"出现的一些有用提示包括: 正确调整资产的物理参数，以确保关节、速度和力的限制在合理范围内，并且机器人的增益已正确调整。检查对机器人施加的动作是否合理，确保它们不会对物体施加过大的力或冲击也是一个好主意。减少物理仿真的时间步长也有助于提高仿真精度和稳定性，此外，增加求解器的迭代次数也有帮助。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:65
msgid "Understanding Training Outputs"
msgstr "理解训练输出"

#: ../../source/overview/reinforcement-learning/training_guide.rst:67
msgid ""
"Each RL library produces its own output data during training. Some libraries"
" are more verbose and generate logs that contain more detailed information "
"on the training process, while others are more compact. In this section, we "
"will explain the common outputs from the RL libraries."
msgstr ""
"每个 RL 库在训练过程中都会产生自己的输出数据。有些库更加冗长，并生成包含更多训练过程详细信息的日志，而其他库则更为简洁。在本节中，我们将解释 RL "
"库的常见输出。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:73
msgid "RL-Games"
msgstr "RL-Games"

#: ../../source/overview/reinforcement-learning/training_guide.rst:75
msgid ""
"For each iteration, RL-Games prints statistics of the data collection, "
"inference, and training performance."
msgstr "对于每次迭代，RL-Games 打印数据收集、推理和训练性能的统计信息。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:81
msgid ""
"``fps step`` refers to the environment step FPS, which includes the applying"
" actions, computing observations, rewards, dones, and resets, as well as "
"stepping simulation."
msgstr "``fps step`` 指的是环境步长 FPS，其中包括应用动作、计算观测、奖励、结束标志、重置以及模拟步进。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:83
msgid ""
"``step and policy inference`` measure everything in ``fps step`` along with "
"the time it takes for the policy inference to compute the actions."
msgstr ""
"``step and policy inference`` 测量 ``fps step`` 中的每一项，以及策略推理计算actions所需的时间。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:85
msgid ""
"``fps total`` measure the above and the time it takes for the training "
"iteration."
msgstr "``fps total`` 测量上述内容以及训练迭代所需的时间。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:87
msgid ""
"At specified intervals, it will also log the current best reward and the "
"path of the intermmediate checkpoints saved to file."
msgstr "在指定的时间间隔内，它还会记录当前最佳奖励和保存到文件的中间检查点路径。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:96
msgid "RSL RL"
msgstr "RSL RL"

#: ../../source/overview/reinforcement-learning/training_guide.rst:98
msgid "For each iteration, RSL RL provides the following output:"
msgstr "对于每次迭代，RSL RL 提供以下输出:"

#: ../../source/overview/reinforcement-learning/training_guide.rst:117
msgid ""
"This output encapsulates the total FPS for data collection, inference, and "
"learning, along with the per-step breakdown for collection and learning time"
" per step. In addition, statistics for the training losses are provided, "
"along with the current average reward and episode length."
msgstr ""
"这个输出包含了数据收集、推理和学习的总 FPS，以及每步的数据收集和学习时间的详细信息。此外，还提供了训练损失的统计数据，以及当前的平均奖励和回合长度。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:120
msgid ""
"In the bottom section, it logs the total number of steps completed so far, "
"the total ieration time for the current ieration, the total overall training"
" time, and the estimated training time to complete the full number of "
"iterations."
msgstr "在底部部分，它记录了迄今为止完成的总步数、当前迭代的总时间、总体训练时间以及完成所有迭代所需的估计训练时间。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:124
msgid "SKRL"
msgstr "SKRL"

#: ../../source/overview/reinforcement-learning/training_guide.rst:126
msgid ""
"SKRL provides a very simplistic output showing the training progress as a "
"percentage of the total number of timesteps (divided by the number of "
"environments). It also includes the total elapsed time so far and the "
"estimated time to complete training."
msgstr ""
"SKRL 提供了一个非常简单的输出，显示训练进度，作为总时间步数的百分比（除以环境数量）。它还包括到目前为止的总经过时间和预计完成训练的时间。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:134
msgid "Stable-Baselines3"
msgstr "Stable-Baselines3"

#: ../../source/overview/reinforcement-learning/training_guide.rst:136
msgid ""
"Stable-Baselines3 provides a detailed output, outlining the rollout "
"statistics, timing, and policy data."
msgstr "Stable-Baselines3 提供了详细的输出，概述了 rollout 统计数据、时间和策略数据。"

#: ../../source/overview/reinforcement-learning/training_guide.rst:163
msgid ""
"Under the ``rollout/`` section, average episode length and reward are logged"
" for the iteration. Under ``time/``, data for the total FPS, number of "
"iterations, total time elapsed, and the total number of timesteps are "
"provided. Finally, under ``train/``, statistics of the training parameters "
"are logged, such as KL, losses, learning rates, and more."
msgstr ""
"在 ``rollout/`` 部分，记录了每次迭代的平均剧集长度和奖励。在 ``time/`` 部分，提供了总 "
"FPS、迭代次数、总耗时和总时间步数的数据。最后，在 ``train/`` 部分，记录了训练参数的统计数据，例如 KL、损失、学习率等。"
