# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-13 11:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:4
msgid "Teleoperation and Imitation Learning with Isaac Lab Mimic"
msgstr "​​Isaac Lab Mimic 中的遥操作与模仿学习​"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:8
msgid "Teleoperation"
msgstr "遥操作"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:10
msgid ""
"We provide interfaces for providing commands in SE(2) and SE(3) space for"
" robot control. In case of SE(2) teleoperation, the returned command is "
"the linear x-y velocity and yaw rate, while in SE(3), the returned "
"command is a 6-D vector representing the change in pose."
msgstr ""
"我们提供接口以便在 SE(2) 和 SE(3) 空间中提供机器人控制命令。在 SE(2) 遥操作的情况下，返回的命令是x-"
"y线速度和yaw角度率，而在 SE(3) 中，返回的命令是一个表示姿态变化的 6-D 向量。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:17
msgid "Presently, Isaac Lab Mimic is only supported in Linux."
msgstr "当前，Isaac Lab Mimic 仅支持 Linux。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:19
msgid "To play inverse kinematics (IK) control with a keyboard device:"
msgstr "要使用键盘设备进行反向运动学（IK）控制: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:25
msgid ""
"For smoother operation and off-axis operation, we recommend using a "
"SpaceMouse as the input device. Providing smoother demonstrations will "
"make it easier for the policy to clone the behavior. To use a SpaceMouse,"
" simply change the teleop device accordingly:"
msgstr ""
"为了更流畅的操作和离轴操作，我们推荐使用 SpaceMouse 作为输入设备。提供更流畅的演示将使得策略更容易克隆行为。要使用 "
"SpaceMouse，只需相应地更改远程操作设备:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:33
msgid ""
"If the SpaceMouse is not detected, you may need to grant additional user "
"permissions by running ``sudo chmod 666 /dev/hidraw<#>`` where ``<#>`` "
"corresponds to the device index of the connected SpaceMouse."
msgstr ""
"如果未检测到 SpaceMouse，您可能需要通过运行 ``sudo chmod 666 /dev/hidraw<#>`` "
"来授予额外的用户权限，其中 ``<#>`` 对应于连接的 SpaceMouse 的设备索引。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:36
msgid ""
"To determine the device index, list all ``hidraw`` devices by running "
"``ls -l /dev/hidraw*``. Identify the device corresponding to the "
"SpaceMouse by running ``cat /sys/class/hidraw/hidraw<#>/device/uevent`` "
"on each of the devices listed from the prior step."
msgstr ""
"为了确定设备索引，运行 ``ls -l /dev/hidraw*`` 列出所有 ``hidraw`` 设备。通过运行 ``cat "
"/sys/class/hidraw/hidraw<#>/device/uevent`` 在每个先前步骤列出的设备上，识别与 SpaceMouse "
"对应的设备。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:40
msgid ""
"We recommend using local deployment of Isaac Lab to use the SpaceMouse. "
"If using container deployment (:ref:`deployment-docker`), you must "
"manually mount the SpaceMouse to the ``isaac-lab-base`` container by "
"adding a ``devices`` attribute with the path to the device in your "
"``docker-compose.yaml`` file:"
msgstr ""
"我们建议使用Isaac Lab的本地部署来使用SpaceMouse。如果使用容器部署 (:ref:`deployment-"
"docker`)，必须通过在 ``docker-compose.yaml`` 文件中添加一个带有设备路径的 ``devices`` "
"属性，将SpaceMouse手动挂载到 ``isaac-lab-base`` 容器中: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:48
msgid "where ``<#>`` is the device index of the connected SpaceMouse."
msgstr "其中 ``<#>`` 是连接的SpaceMouse的设备索引。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:50
msgid ""
"If you are using the IsaacLab + CloudXR container deployment (:ref"
":`cloudxr-teleoperation`), you can add the ``devices`` attribute under "
"the ``services -> isaac-lab-base`` section of the ``docker/docker-compose"
".cloudxr-runtime.patch.yaml`` file."
msgstr ""
"如果您正在使用 IsaacLab + CloudXR 容器部署 (:ref:`cloudxr-teleoperation`)，您可以在 "
"``docker/docker-compose.cloudxr-runtime.patch.yaml`` 文件的 ``services -> "
"isaac-lab-base`` 部分下添加 ``devices`` 属性。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:53
msgid ""
"Isaac Lab is only compatible with the SpaceMouse Wireless and SpaceMouse "
"Compact models from 3Dconnexion."
msgstr "Isaac Lab 仅兼容 3Dconnexion 的 SpaceMouse Wireless 和 SpaceMouse Compact 型号。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:56
msgid ""
"For tasks that benefit from the use of an extended reality (XR) device "
"with hand tracking, Isaac Lab supports using NVIDIA CloudXR to "
"immersively stream the scene to compatible XR devices for teleoperation. "
"Note that when using hand tracking we recommend using the absolute "
"variant of the task (``Isaac-Stack-Cube-Franka-IK-Abs-v0``), which "
"requires the ``handtracking`` device:"
msgstr ""
"对于需要配合扩展现实(XR)设备使用手部追踪的任务，Isaac Lab支持通过NVIDIA "
"CloudXR将场景实时沉浸式串流至兼容XR设备进行遥操作。请注意使用手部追踪时，我们推荐选用任务的绝对变量版本( ``Isaac-Stack-"
"Cube-Franka-IK-Abs-v0`` )，该版本需要配合 ``handtracking`` 设备使用: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:64
msgid ""
"See :ref:`cloudxr-teleoperation` to learn how to use CloudXR and "
"experience teleoperation with Isaac Lab."
msgstr "请查看 :ref:`cloudxr-teleoperation` 了解如何使用 CloudXR 并体验 Isaac Lab 的远程操作。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:67
msgid ""
"The script prints the teleoperation events configured. For keyboard, "
"these are as follows:"
msgstr "脚本打印配置的遥操作事件。对于键盘，它们如下: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:82
msgid "For SpaceMouse, these are as follows:"
msgstr "对于 SpaceMouse，具体如下:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:93
msgid ""
"The next section describes how teleoperation devices can be used for data"
" collection for imitation learning."
msgstr "下一节描述了如何使用遥操作设备进行模仿学习的数据收集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:97
msgid "Imitation Learning with Isaac Lab Mimic"
msgstr "​​Isaac Lab Mimic 中的模仿学习​"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:99
msgid ""
"Using the teleoperation devices, it is also possible to collect data for "
"learning from demonstrations (LfD). For this, we provide scripts to "
"collect data into the open HDF5 format."
msgstr "使用遥操作设备，还可以收集用于示范学习（LfD）的数据。为此，我们提供脚本将数据收集到开放的 HDF5 格式中。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:103
msgid "Collecting demonstrations"
msgstr "收集演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:105
msgid ""
"To collect demonstrations with teleoperation for the environment ``Isaac-"
"Stack-Cube-Franka-IK-Rel-v0``, use the following commands:"
msgstr "要收集环境 ``Isaac-Stack-Cube-Franka-IK-Rel-v0`` 的遥操作示范，请使用以下命令:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:120
msgid ""
"The order of the stacked cubes should be blue (bottom), red (middle), "
"green (top)."
msgstr "堆叠立方体的顺序应为蓝色（底部）、红色（中间）、绿色（顶部）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:124
msgid ""
"When using an XR device, we suggest collecting demonstrations with the "
"``Isaac-Stack-Cube-Frank-IK-Abs-v0`` version of the task and "
"``--teleop_device handtracking``, which controls the end effector using "
"the absolute position of the hand."
msgstr ""
"在使用XR设备时，我们建议使用任务的 ``Isaac-Stack-Cube-Franka-IK-Abs-v0`` 版本和 "
"``--teleop_device handtracking`` 来收集演示，该命令可控制末端执行器使用手部的绝对位置。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:126
msgid ""
"About 10 successful demonstrations are required in order for the "
"following steps to succeed."
msgstr "为了后续步骤成功，通常需要大约 10 次成功演示。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:128
msgid ""
"Here are some tips to perform demonstrations that lead to successful "
"policy training:"
msgstr "以下是一些执行演示并成功进行策略训练的技巧:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:130
msgid ""
"Keep demonstrations short. Shorter demonstrations mean fewer decisions "
"for the policy, making training easier."
msgstr "保持演示简短。简短的演示意味着策略需要做出的决策更少，从而使训练变得更容易。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:131
msgid ""
"Take a direct path. Do not follow along arbitrary axis, but move straight"
" toward the goal."
msgstr "走一条直接的路径。不要沿着任意轴线移动，而是直线朝着目标前进。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:132
msgid ""
"Do not pause. Perform smooth, continuous motions instead. It is not "
"obvious for a policy why and when to pause, hence continuous motions are "
"easier to learn."
msgstr "不要暂停。请执行平滑、连续的动作。由于策略并不明显地解释何时以及为何要暂停，因此连续的动作更容易学习。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:134
msgid ""
"If, while performing a demonstration, a mistake is made, or the current "
"demonstration should not be recorded for some other reason, press the "
"``R`` key to discard the current demonstration, and reset to a new "
"starting position."
msgstr "如果在进行演示时出现错误，或者当前的演示由于其他原因不应被记录，请按 ``R`` 键丢弃当前演示，并重置到新的起始位置。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:137
#: ../../source/overview/imitation-learning/teleop_imitation.rst:431
msgid ""
"Non-determinism may be observed during replay as physics in IsaacLab are "
"not determimnistically reproducible when using ``env.reset``."
msgstr "非确定性可能会在回放过程中观察到，因为在使用 ``env.reset`` 时，IsaacLab 中的物理现象无法确定性地重现。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:140
msgid "Pre-recorded demonstrations"
msgstr "预先录制的演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:142
msgid ""
"We provide a pre-recorded ``dataset.hdf5`` containing 10 human "
"demonstrations for ``Isaac-Stack-Cube-Franka-IK-Rel-v0`` here: `[Franka "
"Dataset] <https://omniverse-content-production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/franka_stack_datasets/dataset.hdf5>`__."
" This dataset may be downloaded and used in the remaining tutorial steps "
"if you do not wish to collect your own demonstrations."
msgstr ""
"我们提供了一个预先录制的 ``dataset.hdf5`` ，包含 10 个关于 ``Isaac-Stack-Cube-Franka-IK-"
"Rel-v0`` 的人类示范，您可以在这里下载: `[Franka Dataset] <https://omniverse-content-"
"production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/franka_stack_datasets/dataset.hdf5>`__"
" 。如果您不希望收集自己的示范，可以在接下来的教程步骤中下载并使用此数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:147
msgid "Use of the pre-recorded dataset is optional."
msgstr "使用预录制的数据集是可选的。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:152
msgid "Generating additional demonstrations with Isaac Lab Mimic"
msgstr "使用 Isaac Lab Mimic 生成额外的演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:154
msgid "Additional demonstrations can be generated using Isaac Lab Mimic."
msgstr "额外的演示可以使用 Isaac Lab Mimic 生成。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:156
msgid ""
"Isaac Lab Mimic is a feature in Isaac Lab that allows generation of "
"additional demonstrations automatically, allowing a policy to learn "
"successfully even from just a handful of manual demonstrations."
msgstr ""
"Isaac Lab Mimic 是 Isaac Lab "
"中的一个功能，允许自动生成额外的示范，从而使得一个策略即使仅通过少量的手动示范也能够成功学习。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:158
msgid ""
"In the following example, we will show how to use Isaac Lab Mimic to "
"generate additional demonstrations that can be used to train either a "
"state-based policy (using the ``Isaac-Stack-Cube-Franka-IK-Rel-Mimic-v0``"
" environment) or visuomotor policy (using the ``Isaac-Stack-Cube-Franka-"
"IK-Rel-Visuomotor-Mimic-v0`` environment)."
msgstr ""
"在下面的示例中，我们将展示如何使用Isaac Lab Mimic生成额外的演示，可用于训练基于状态的策略 (使用 ``Isaac-Stack-"
"Cube-Franka-IK-Rel-Mimic-v0`` 环境)或视觉运动策略 (使用 ``Isaac-Stack-Cube-Franka-"
"IK-Rel-Visuomotor-Mimic-v0`` 环境)。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:162
msgid ""
"The following commands are run using CPU mode as a small number of envs "
"are used which are I/O bound rather than compute bound."
msgstr "以下命令使用 CPU 模式运行，因为使用的环境数量较少，属于 I/O 受限而非计算受限。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:166
msgid ""
"All commands in the following sections must keep a consistent policy "
"type. For example, if choosing to use a state-based policy, then all "
"commands used should be from the \"State-based policy\" tab."
msgstr "以下各节中的所有命令必须保持一致的策略类型。例如，如果选择使用基于状态的策略，那么所有使用的命令都应来自 \"基于状态的策略\" 选项卡。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:168
msgid ""
"In order to use Isaac Lab Mimic with the recorded dataset, first annotate"
" the subtasks in the recording:"
msgstr "为了使用 Isaac Lab Mimic 和录制的数据集，首先在录制中标注子任务:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst
msgid "State-based policy"
msgstr "基于状态的策略"

#: ../../source/overview/imitation-learning/teleop_imitation.rst
msgid "Visuomotor policy"
msgstr "视觉运动策略"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:192
msgid "Then, use Isaac Lab Mimic to generate some additional demonstrations:"
msgstr "然后，使用 Isaac Lab Mimic 生成一些额外的示范:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:217
msgid ""
"The output_file of the ``annotate_demos.py`` script is the input_file to "
"the ``generate_dataset.py`` script"
msgstr ""
"``annotate_demos.py`` 脚本的 output_file 是 ``generate_dataset.py`` 脚本的 "
"input_file。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:219
msgid ""
"Inspect the output of generated data (filename: "
"``generated_dataset_small.hdf5``), and if satisfactory, generate the full"
" dataset:"
msgstr "检查生成的数据输出（文件名: ``generated_dataset_small.hdf5`` ），如果满意，生成完整数据集:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:243
msgid ""
"The number of demonstrations can be increased or decreased, 1000 "
"demonstrations have been shown to provide good training results for this "
"task."
msgstr "展示的数量可以增加或减少，1000 个展示已被证明能够为此任务提供良好的训练结果。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:245
msgid ""
"Additionally, the number of environments in the ``--num_envs`` parameter "
"can be adjusted to speed up data generation. The suggested number of 10 "
"can be executed on a moderate laptop GPU. On a more powerful desktop "
"machine, use a larger number of environments for a significant speedup of"
" this step."
msgstr ""
"此外，可以调整 ``--num_envs`` 参数中的环境数量，以加快数据生成。建议的数量为10，可以在中等规模的笔记本 GPU "
"上执行。在更强大的台式机上，使用更多的环境可以显著加快这一步的速度。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:250
msgid "Robomimic setup"
msgstr "Robomimic 设置"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:252
msgid ""
"As an example, we will train a BC agent implemented in `Robomimic "
"<https://robomimic.github.io/>`__ to train a policy. Any other framework "
"or training method could be used."
msgstr ""
"作为一个例子，我们将训练一个在 `Robomimic <https://robomimic.github.io/>`__ 中实现的 BC "
"智能体来训练一个策略。可以使用任何其他框架或训练方法。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:254
msgid "To install the robomimic framework, use the following commands:"
msgstr "要安装 robomimic 框架，请使用以下命令:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:264
msgid "Training an agent"
msgstr "训练一个智能体"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:266
msgid ""
"Using the Mimic generated data we can now train a state-based BC agent "
"for ``Isaac-Stack-Cube-Franka-IK-Rel-v0``, or a visuomotor BC agent for "
"``Isaac-Stack-Cube-Franka-IK-Rel-Visuomotor-v0``:"
msgstr ""
"使用Mimic生成的数据，现在我们可以训练一个针对 ``Isaac-Stack-Cube-Franka-IK-Rel-v0`` "
"的基于状态的BC智能体，或者为 ``Isaac-Stack-Cube-Franka-IK-Rel-Visuomotor-v0`` "
"训练一个视觉运动BC智能体: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:290
#: ../../source/overview/imitation-learning/teleop_imitation.rst:509
msgid ""
"By default the trained models and logs will be saved to "
"``IssacLab/logs/robomimic``."
msgstr "默认情况下，训练的模型和日志将保存到 ``IssacLab/logs/robomimic`` 中。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:293
msgid "Visualizing results"
msgstr "可视化结果"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:297
msgid "**Important: Testing Multiple Checkpoint Epochs**"
msgstr "**重要提示：测试多个检查点轮次**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:299
msgid ""
"When evaluating policy performance, it is common for different training "
"epochs to yield significantly different results. If you don't see the "
"expected performance, **always test policies from various epochs** (not "
"just the final checkpoint) to find the best-performing model. Model "
"performance can vary substantially across training, and the final epoch "
"is not always optimal."
msgstr ""
"在评估策略性能时，不同的训练轮次通常会产生显著不同的结果。如果您没有看到预期的性能，"
"**请始终测试来自不同轮次的策略**（而不仅仅是最后一个检查点）以找到性能最佳的模型。"
"模型性能在整个训练过程中可能存在显著差异，最后一个轮次并不总是最优的。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:304
msgid ""
"By inferencing using the generated model, we can visualize the results of"
" the policy:"
msgstr "通过使用生成的模型进行推理，我们可以可视化策略的结果:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:329
msgid ""
"**If you don't see expected performance results:** Test policies from "
"multiple checkpoint epochs, not just the final one. Policy performance "
"can vary significantly across training epochs, and intermediate "
"checkpoints often outperform the final model."
msgstr ""
"**如果您没有看到预期的性能结果：** 请测试来自多个检查点轮次的策略，而不仅仅是最后一个。"
"策略性能在不同训练轮次之间可能存在显著差异，中间检查点往往优于最终模型。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:334
msgid "**Expected Success Rates and Timings for Franka Cube Stack Task**"
msgstr "**Franka 立方体堆叠任务的预期成功率和时间**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:336
msgid "Data generation success rate: ~50% (for both state + visuomotor)"
msgstr "数据生成成功率: ~50%（状态和视觉运动均是）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:337
msgid ""
"Data generation time: ~30 mins for state, ~4 hours for visuomotor (varies"
" based on num envs the user runs)"
msgstr "数据生成时间: 状态约 30 分钟，视觉运动约 4 小时（根据用户运行的环境数量而变化）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:338
msgid ""
"BC RNN training time: 1000 epochs + ~30 mins (for state), 600 epochs + ~6"
" hours (for visuomotor)"
msgstr "BC RNN 训练时间: 1000 轮次约 30 分钟（状态），600 轮次约 6 小时（视觉运动）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:339
msgid "BC RNN policy success rate: ~40-60% (for both state + visuomotor)"
msgstr "BC RNN 策略成功率: ~40-60%（状态和视觉运动均是）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:340
msgid ""
"**Recommendation:** Evaluate checkpoints from various epochs throughout "
"training to identify the best-performing model"
msgstr ""
"**建议：** 在整个训练过程中评估来自不同轮次的检查点，以识别性能最佳的模型"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:344
msgid "Demo 1: Data Generation and Policy Training for a Humanoid Robot"
msgstr "演示1: 人形机器人的数据生成和策略训练"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:346
#: ../../source/overview/imitation-learning/teleop_imitation.rst:536
msgid "GR-1 humanoid robot performing a pick and place task"
msgstr "GR-1人形机器人执行拾取和放置任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:353
msgid ""
"Isaac Lab Mimic supports data generation for robots with multiple end "
"effectors. In the following demonstration, we will show how to generate "
"data to train a Fourier GR-1 humanoid robot to perform a pick and place "
"task."
msgstr ""
"Isaac Lab Mimic支持对多个末端执行器的机器人生成数据。在以下演示中，我们将展示如何生成数据以训练一个Fourier "
"GR-1人形机器人执行拾取和放置任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:357
msgid "Optional: Collect and annotate demonstrations"
msgstr "可选: 收集和标注演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:360
msgid "Collect human demonstrations"
msgstr "收集人类演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:363
msgid ""
"Data collection for the GR-1 humanoid robot environment requires use of "
"an Apple Vision Pro headset. If you do not have access to an Apple Vision"
" Pro, you may skip this step and continue on to the next step: `Generate "
"the dataset`_. A pre-recorded annotated dataset is provided in the next "
"step."
msgstr ""
"GR-1人形机器人环境的数据收集需要使用Apple Vision Pro头戴式显示器。如果您无法使用Apple Vision "
"Pro，可以跳过此步骤，直接进入下一步: `生成数据集`_ 。下一步将提供预先录制的带标注数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:368
msgid ""
"The GR1 scene utilizes the wrist poses from the Apple Vision Pro (AVP) as"
" setpoints for a differential IK controller (Pink-IK). The differential "
"IK controller requires the user's wrist pose to be close to the robot's "
"initial or current pose for optimal performance. Rapid movements of the "
"user's wrist may cause it to deviate significantly from the goal state, "
"which could prevent the IK controller from finding the optimal solution. "
"This may result in a mismatch between the user's wrist and the robot's "
"wrist. You can increase the gain of all the `Pink-IK controller's "
"FrameTasks <https://github.com/isaac-"
"sim/IsaacLab/blob/main/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/pick_place/pickplace_gr1t2_env_cfg.py>`__"
" to track the AVP wrist poses with lower latency. However, this may lead "
"to more jerky motion. Separately, the finger joints of the robot are "
"retargeted to the user's finger joints using the `dex-retargeting "
"<https://github.com/dexsuite/dex-retargeting>`_ library."
msgstr ""
"GR1场景利用Apple Vision Pro (AVP)的手腕姿势作为微分 IK 控制器(Pink-IK)的设定点。微分 IK "
"控制器要求用户的手腕姿势接近机器人的初始或当前姿势，以获得最佳性能。用户的手腕快速移动可能导致其明显偏离目标状态，这可能会阻止 IK "
"控制器找到最佳解决方案。这可能导致用户的手腕和机器人手腕之间存在不匹配。您可以增加所有 `Pink-IK控制器的帧任务 "
"<https://github.com/isaac-"
"sim/IsaacLab/blob/main/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/pick_place/pickplace_gr1t2_env_cfg.py>`__"
" 的增益以以较低的延迟跟踪AVP手腕姿势。但是，这可能会导致更多的抖动运动。另外，机器人的手指关节通过 `dex-retargeting "
"<https://github.com/dexsuite/dex-retargeting>`_ 库重新定位到用户的手指关节。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:376
msgid ""
"Set up the CloudXR Runtime and Apple Vision Pro for teleoperation by "
"following the steps in :ref:`cloudxr-teleoperation`. CPU simulation is "
"used in the following steps for better XR performance when running a "
"single environment."
msgstr ""
"按照 :ref:`cloudxr-teleoperation` 中的步骤设置CloudXR运行时和Apple Vision "
"Pro以进行远程操作。在运行单个环境时，使用CPU仿真可获得更好的XR性能。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:379
msgid ""
"Collect a set of human demonstrations. A success demo requires the object"
" to be placed in the bin and for the robot's right arm to be retracted to"
" the starting position."
msgstr "收集一组人类演示。成功的演示需要将物体放置在箱子中，并使机器人的右臂缩回到起始位置。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:382
msgid ""
"The Isaac Lab Mimic Env GR-1 humanoid robot is set up such that the left "
"hand has a single subtask, while the right hand has two subtasks. The "
"first subtask involves the right hand remaining idle while the left hand "
"picks up and moves the object to the position where the right hand will "
"grasp it. This setup allows Isaac Lab Mimic to interpolate the right "
"hand's trajectory accurately by using the object's pose, especially when "
"poses are randomized during data generation. Therefore, avoid moving the "
"right hand while the left hand picks up the object and brings it to a "
"stable position."
msgstr ""
"Isaac Lab Mimic Env GR-1 人形机器人被设置如下: "
"左手有一个子任务，而右手有两个子任务。第一个子任务是右手保持空闲，左手拾起并移动物体到右手将要抓取的位置。这种设置使得Isaac Lab "
"Mimic "
"能够通过使用物体的姿势准确地插值右手的轨迹，尤其是在数据生成过程中姿势被随机化的情况下。因此，在左手拾起物体并将其带到稳定位置时，请避免移动右手。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:396
msgid "|good_demo| |bad_demo|"
msgstr "|good_demo| |bad_demo|"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:388
msgid "GR-1 humanoid robot performing a good pick and place demonstration"
msgstr "GR-1 人形机器人完成了一个良好的拾取和放置演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:392
msgid "GR-1 humanoid robot performing a bad pick and place demonstration"
msgstr "GR-1 人形机器人完成了一个糟糕的拾取和放置演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:400
msgid ""
"Left: A good human demonstration with smooth and steady motion. Right: A "
"bad demonstration with jerky and exaggerated motion."
msgstr "左: 一个演示人类动作流畅稳定的好例子。右: 一个动作生硬夸张的糟糕示范。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:401
msgid "Collect five demonstrations by running the following command:"
msgstr "通过运行以下命令收集五个演示: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:413
msgid ""
"We also provide a GR-1 pick and place task with waist degrees-of-freedom "
"enabled ``Isaac-PickPlace-GR1T2-WaistEnabled-Abs-v0`` (see "
":ref:`environments` for details on the available environments, including "
"the GR1 Waist Enabled variant). The same command above applies but with "
"the task name changed to ``Isaac-PickPlace-GR1T2-WaistEnabled-Abs-v0``."
msgstr ""
"我们还提供了一个启用腰部自由度的 GR-1 拾取和放置任务 ``Isaac-PickPlace-GR1T2-WaistEnabled-"
"Abs-v0`` （有关可用环境的详细信息，包括 启用腰部的GR1变体，请参阅 :ref:`environments` "
"）。适用上述相同的命令，但任务名称更改为 ``Isaac-PickPlace-GR1T2-WaistEnabled-Abs-v0``."

#: ../../source/overview/imitation-learning/teleop_imitation.rst:416
msgid ""
"If a demo fails during data collection, the environment can be reset "
"using the teleoperation controls panel in the XR teleop client on the "
"Apple Vision Pro or via voice control by saying \"reset\". See :ref"
":`teleoperate-apple-vision-pro` for more details."
msgstr ""
"如果在数据收集过程中演示失败，可以使用Apple Vision Pro上的XR远程操控客户端中的远程操控控制面板或通过说 \"reset\" "
"来重置环境。查看 :ref:`teleoperate-apple-vision-pro` 以获取更多详细信息。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:419
msgid ""
"The robot uses simplified collision meshes for physics calculations that "
"differ from the detailed visual meshes displayed in the simulation. Due "
"to this difference, you may occasionally observe visual artifacts where "
"parts of the robot appear to penetrate other objects or itself, even "
"though proper collision handling is occurring in the physics simulation."
msgstr "机器人使用简化的碰撞网格进行物理计算，该网格与仿真中显示的详细可视网格不同。由于这种差异，您可能偶尔会观察到可视图中机器人的某些部分似乎穿透其他物体或其自身，尽管在物理仿真中正常处理碰撞。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:421
msgid ""
"You can replay the collected demonstrations by running the following "
"command:"
msgstr "您可以通过运行以下命令重现收集的演示: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:435
msgid "Annotate the demonstrations"
msgstr "标注演示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:437
msgid ""
"Unlike the prior Franka stacking task, the GR-1 pick and place task uses "
"manual annotation to define subtasks."
msgstr "与之前的 Franka 堆叠任务不同，GR-1 拾取和放置任务使用手动标注来定义子任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:439
msgid ""
"The pick and place task has one subtask for the left arm (pick) and two "
"subtasks for the right arm (idle, place). Annotations denote the end of a"
" subtask. For the pick and place task, this means there are no "
"annotations for the left arm and one annotation for the right arm (the "
"end of the final subtask is always implicit)."
msgstr ""
"拾取和放置任务有一个左臂子任务（pick）和两个右臂子任务（idle、place）。 标注表示子任务的结束。 "
"对于拾取和放置任务，这意味着左臂子任务没有标注，右臂子任务有一个标注（最后子任务的结束始终是隐式的）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:442
msgid ""
"Each demo requires a single annotation between the first and second "
"subtask of the right arm. This annotation (\"S\" button press) should be "
"done when the right robot arm finishes the \"idle\" subtask and begins to"
" move towards the target object. An example of a correct annotation is "
"shown below:"
msgstr ""
"每个演示都需要在右臂的第一个和第二个子任务之间进行一次单独的标注。此标注( \"S\" 按钮按下)应在右侧机器人手臂完成 \"idle\" "
"子任务并开始朝着目标物体移动时完成。下面显示了一个正确标注的示例: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:449
msgid "Annotate the demonstrations by running the following command:"
msgstr "通过运行以下命令标注演示: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:461
msgid ""
"The script prints the keyboard commands for manual annotation and the "
"current subtask being annotated:"
msgstr "脚本打印用于手动标注的键盘命令以及当前正在标注的子任务: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:477
msgid ""
"If the object does not get placed in the bin during annotation, you can "
"press \"N\" to replay the episode and annotate again. Or you can press "
"\"Q\" to skip the episode and annotate the next one."
msgstr "如果在标注过程中物体未放入箱子，则可以按 \"N\" 重新播放该事件并重新标注。或者您可以按 \"Q\" 跳过该事件并标注下一个。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:480
msgid "Generate the dataset"
msgstr "生成数据集"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:482
msgid ""
"If you skipped the prior collection and annotation step, download the "
"pre-recorded annotated dataset ``dataset_annotated_gr1.hdf5`` from here: "
"`[Annotated GR1 Dataset] <https://omniverse-content-production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/pick_place_datasets/dataset_annotated_gr1.hdf5>`_."
" Place the file under ``IsaacLab/datasets`` and run the following command"
" to generate a new dataset with 1000 demonstrations."
msgstr ""
"如果您跳过了之前的收集和标注步骤，请从这里下载预先记录的带标注数据集 ``dataset_annotated_gr1.hdf5``: "
"`[Annotated GR1 Dataset] <https://omniverse-content-production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/pick_place_datasets/dataset_annotated_gr1.hdf5>`_"
" 。将文件放置在 ``IsaacLab/datasets`` 下，并运行以下命令以生成一个包含 1000 个演示的新数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:493
#: ../../source/overview/imitation-learning/teleop_imitation.rst:843
msgid "Train a policy"
msgstr "训练一个策略"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:495
msgid ""
"Use `Robomimic <https://robomimic.github.io/>`__ to train a policy for "
"the generated dataset."
msgstr "使用 `Robomimic <https://robomimic.github.io/>`__ 为生成的数据集训练一个策略。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:504
#: ../../source/overview/imitation-learning/teleop_imitation.rst:854
msgid ""
"The training script will normalize the actions in the dataset to the "
"range [-1, 1]. The normalization parameters are saved in the model "
"directory under "
"``PATH_TO_MODEL_DIRECTORY/logs/normalization_params.txt``. Record the "
"normalization parameters for later use in the visualization step."
msgstr ""
"训练脚本将标准化数据集中的操作到范围[-1, 1]。标准化参数保存在模型目录下的 "
"``PATH_TO_MODEL_DIRECTORY/logs/normalization_params.txt`` "
"文件中。记录标准化参数以备后续在可视化步骤中使用。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:512
#: ../../source/overview/imitation-learning/teleop_imitation.rst:656
#: ../../source/overview/imitation-learning/teleop_imitation.rst:868
msgid "Visualize the results"
msgstr "可视化结果"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:514
#: ../../source/overview/imitation-learning/teleop_imitation.rst:870
msgid ""
"Visualize the results of the trained policy by running the following "
"command, using the normalization parameters recorded in the prior "
"training step:"
msgstr "通过运行以下命令，使用之前训练步骤中记录的标准化参数来可视化训练策略的结果: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:529
#: ../../source/overview/imitation-learning/teleop_imitation.rst:673
#: ../../source/overview/imitation-learning/teleop_imitation.rst:887
msgid ""
"Change the ``NORM_FACTOR`` in the above command with the values generated"
" in the training step."
msgstr "将上述命令中的 ``NORM_FACTOR`` 更改为训练步骤中生成的值。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:533
msgid ""
"**If you don't see expected performance results:** It is critical to test"
" policies from various checkpoint epochs. Performance can vary "
"significantly between epochs, and the best-performing checkpoint is often"
" not the final one."
msgstr ""
"**如果您没有看到预期的性能结果：** 测试来自不同检查点轮次的策略至关重要。"
"不同轮次之间的性能可能存在显著差异，性能最佳的检查点往往不是最后一个。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:542
#: ../../source/overview/imitation-learning/teleop_imitation.rst:686
msgid "The trained policy performing the pick and place task in Isaac Lab."
msgstr "在 Isaac Lab 中执行拾取和放置任务的训练策略。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:546
msgid "**Expected Success Rates and Timings for Pick and Place GR1T2 Task**"
msgstr "**GR1T2 拾取和放置任务的预期成功率和时间**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:548
#: ../../source/overview/imitation-learning/teleop_imitation.rst:692
#: ../../source/overview/imitation-learning/teleop_imitation.rst:906
msgid ""
"Success rate for data generation depends on the quality of human "
"demonstrations (how well the user performs them) and dataset annotation "
"quality. Both data generation and downstream policy success are sensitive"
" to these factors and can show high variance. See :ref:`Common Pitfalls "
"when Generating Data <common-pitfalls-generating-data>` for tips to "
"improve your dataset."
msgstr ""
"数据生成的成功率取决于人类演示的质量（用户执行的好坏程度）和数据集标注质量。数据生成和下游策略成功对这些因素都很敏感，可能会出现较大差异。请参阅 "
":ref:`Common Pitfalls when Generating Data <common-pitfalls-generating-"
"data>` 获取改进数据集的提示。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:549
#, python-format
msgid ""
"Data generation success for this task is typically 65-80% over 1000 "
"demonstrations, taking 18-40 minutes depending on GPU hardware and "
"success rate (19 minutes on a RTX ADA 6000 @ 80% success rate)."
msgstr ""
"此任务的数据生成成功率通常为 65-80%（1000 个演示），需要 18-40 分钟，具体取决于 GPU 硬件和成功率（在 RTX ADA "
"6000 上成功率 80% 时为 19 分钟）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:550
msgid ""
"Behavior Cloning (BC) policy success is typically 75-86% (evaluated on 50"
" rollouts) when trained on 1000 generated demonstrations for 2000 epochs "
"(default), depending on demonstration quality. Training takes "
"approximately 29 minutes on a RTX ADA 6000."
msgstr ""
"行为克隆（BC）策略成功率通常为 75-86%（在 50 次展示中评估），基于 1000 个生成的演示训练 2000 "
"个轮次（默认），具体取决于演示质量。在 RTX ADA 6000 上训练大约需要 29 分钟。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:551
#: ../../source/overview/imitation-learning/teleop_imitation.rst:695
msgid ""
"**Recommendation:** Train for 2000 epochs with 1000 generated "
"demonstrations, and **evaluate multiple checkpoints saved between the "
"1000th and 2000th epochs** to select the best-performing policy. Testing "
"various epochs is essential for finding optimal performance."
msgstr ""
"**建议：** 使用 1000 个生成的演示训练 2000 个轮次，并 **评估保存在第 1000 到第 2000 "
"个轮次之间的多个检查点**，以选择性能最佳的策略。测试不同轮次对于找到最优性能至关重要。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:555
msgid ""
"Demo 2: Data Generation and Policy Training for Humanoid Robot "
"Locomanipulation with Unitree G1"
msgstr "演示 2: 使用 Unitree G1 进行人形机器人运动操作的数据生成和策略训练"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:557
msgid ""
"In this demo, we showcase the integration of locomotion and manipulation "
"capabilities within a single humanoid robot system. This locomanipulation"
" environment enables data collection for complex tasks that combine "
"navigation and object manipulation. The demonstration follows a multi-"
"step process: first, it generates pick and place tasks similar to Demo 1,"
" then introduces a navigation component that uses specialized scripts to "
"generate scenes where the humanoid robot must move from point A to point "
"B. The robot picks up an object at the initial location (point A) and "
"places it at the target destination (point B)."
msgstr ""
"在本演示中，我们展示了在单个人形机器人系统中集成运动和操作能力。这个运动操作环境能够为结合导航和物体操作的复杂任务收集数据。演示遵循多步骤过程: "
"首先生成类似于演示 1 的拾取和放置任务，然后引入导航组件，使用专门的脚本生成场景，其中人形机器人必须从 A 点移动到 B "
"点。机器人在初始位置（A 点）拾取物体，并将其放置在目标位置（B 点）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:563
msgid "G1 humanoid robot with locomanipulation performing a pick and place task"
msgstr "具有运动操作能力的 G1 人形机器人执行拾取和放置任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:570
msgid "**Locomotion policy training**"
msgstr "**运动策略训练**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:572
msgid ""
"The locomotion policy used in this integration example was trained using "
"the `AGILE <https://github.com/nvidia-isaac/WBC-AGILE>`__ framework. "
"AGILE is an officially supported humanoid control training pipeline that "
"leverages the manager based environment in Isaac Lab. It will also be "
"seamlessly integrated with other evaluation and deployment tools across "
"Isaac products. This allows teams to rely on a single, maintained stack "
"covering all necessary infrastructure and tooling for policy training, "
"with easy export to real-world deployment. The AGILE repository contains "
"updated pre-trained policies with separate upper and lower body policies "
"for flexibtility. They have been verified in the real world and can be "
"directly deployed. Users can also train their own locomotion or whole-"
"body control policies using the AGILE framework."
msgstr ""
"此集成示例中使用的运动策略是使用 `AGILE <https://github.com/nvidia-isaac/WBC-AGILE>`__ "
"框架训练的。AGILE 是一个官方支持的人形机器人控制训练流水线，它利用了 Isaac Lab 中基于管理器的环境。"
"它还将与 Isaac 产品中的其他评估和部署工具无缝集成。这使得团队能够依赖单一的、维护良好的技术栈，"
"涵盖策略训练所需的所有基础设施和工具，并可轻松导出到真实世界部署。AGILE 仓库包含更新的预训练策略，"
"具有独立的上半身和下半身策略以提供灵活性。这些策略已在真实世界中验证，可以直接部署。"
"用户还可以使用 AGILE 框架训练自己的运动或全身控制策略。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:580
msgid "Generate the manipulation dataset"
msgstr "生成操作数据集"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:582
msgid ""
"The same data generation and policy training steps from Demo 1.0 can be "
"applied to the G1 humanoid robot with locomanipulation capabilities. This"
" demonstration shows how to train a G1 robot to perform pick and place "
"tasks with full-body locomotion and manipulation."
msgstr ""
"演示 1.0 中相同的数据生成和策略训练步骤可以应用于具有运动操作能力的 G1 人形机器人。本演示展示了如何训练 G1 "
"机器人执行包含全身运动和操作的拾取和放置任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:585
msgid ""
"The process follows the same workflow as Demo 1.0, but uses the ``Isaac-"
"PickPlace-Locomanipulation-G1-Abs-v0`` task environment."
msgstr ""
"该过程遵循与演示 1.0 相同的工作流程，但使用 ``Isaac-PickPlace-Locomanipulation-G1-Abs-v0`` "
"任务环境。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:587
msgid ""
"Follow the same data collection, annotation, and generation process as "
"demonstrated in Demo 1.0, but adapted for the G1 locomanipulation task."
msgstr "遵循演示 1.0 中演示的相同数据收集、标注和生成过程，但针对 G1 运动操作任务进行了调整。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:591
msgid ""
"If desired, data collection and annotation can be done using the same "
"commands as the prior examples for validation of the dataset."
msgstr "如果需要，可以使用与之前示例相同的命令进行数据收集和标注以验证数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:593
msgid ""
"The G1 robot with locomanipulation capabilities combines full-body "
"locomotion with manipulation to perform pick and place tasks."
msgstr "具有运动操作能力的 G1 机器人结合全身运动和操作来执行拾取和放置任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:595
msgid ""
"**Note that the following commands are only for your reference and "
"dataset validation purposes - they are not required for this demo.**"
msgstr "**注意，以下命令仅供参考和数据集验证之用 - 对本演示不是必需的。**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:597
#: ../../source/overview/imitation-learning/teleop_imitation.rst:795
msgid "To collect demonstrations:"
msgstr "收集演示: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:610
msgid ""
"Depending on how the Apple Vision Pro app was initialized, the hands of "
"the operator might be very far up or far down compared to the hands of "
"the G1 robot. If this is the case, you can click **Stop AR** in the AR "
"tab in Isaac Lab, and move the AR Anchor prim. Adjust it down to bring "
"the hands of the operator lower, and up to bring them higher. Click "
"**Start AR** to resume teleoperation session. Make sure to match the "
"hands of the robot before clicking **Play** in the Apple Vision Pro, "
"otherwise there will be an undesired large force generated initially."
msgstr ""
"根据 Apple Vision Pro 应用程序的初始化方式，操作员的手可能与 G1 机器人的手相比非常高或非常低。如果是这种情况，您可以在 "
"Isaac Lab 的 AR 选项卡中单击 **Stop AR** ，然后移动 AR Anchor "
"prim。向下调整以降低操作员的手部高度，向上调整以提高高度。单击 **Start AR** 恢复远程操作会话。在 Apple Vision "
"Pro 中单击 **Play** 之前，请确保匹配机器人的手部，否则最初会产生不期望的大力。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:612
msgid "You can replay the collected demonstrations by running:"
msgstr "您可以通过运行以下命令重现收集的演示:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:621
#: ../../source/overview/imitation-learning/teleop_imitation.rst:808
msgid "To annotate the demonstrations:"
msgstr "标注演示: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:632
msgid ""
"If you skipped the prior collection and annotation step, download the "
"pre-recorded annotated dataset ``dataset_annotated_g1_locomanip.hdf5`` "
"from here: `[Annotated G1 Dataset] <https://omniverse-content-"
"production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/pick_place_datasets/dataset_annotated_g1_locomanip.hdf5>`_."
" Place the file under ``IsaacLab/datasets`` and run the following command"
" to generate a new dataset with 1000 demonstrations."
msgstr ""
"如果您跳过了之前的收集和标注步骤，请从这里下载预先记录的带标注数据集 "
"``dataset_annotated_g1_locomanip.hdf5``: `[Annotated G1 Dataset] <https"
"://omniverse-content-production.s3-us-"
"west-2.amazonaws.com/Assets/Isaac/5.1/Isaac/IsaacLab/Mimic/pick_place_datasets/dataset_annotated_g1_locomanip.hdf5>`_"
" 。将文件放置在 ``IsaacLab/datasets`` 下，并运行以下命令以生成一个包含 1000 个演示的新数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:644
msgid "Train a manipulation-only policy"
msgstr "训练仅操作的策略"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:646
msgid ""
"At this point you can train a policy that only performs manipulation "
"tasks using the generated dataset:"
msgstr "此时，您可以使用生成的数据集训练一个仅执行操作任务的策略:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:658
msgid "Visualize the trained policy performance:"
msgstr "可视化训练策略的性能:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:677
msgid ""
"**If you don't see expected performance results:** Always test policies "
"from various checkpoint epochs. Different epochs can produce "
"significantly different results, so evaluate multiple checkpoints to find"
" the optimal model."
msgstr ""
"**如果您没有看到预期的性能结果：** 请始终测试来自不同检查点轮次的策略。"
"不同轮次可能会产生显著不同的结果，因此请评估多个检查点以找到最优模型。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:680
msgid "G1 humanoid robot performing a pick and place task"
msgstr "G1 人形机器人执行拾取和放置任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:690
msgid ""
"**Expected Success Rates and Timings for Locomanipulation Pick and Place "
"Task**"
msgstr "**运动操作拾取和放置任务的预期成功率和时间**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:693
#, python-format
msgid ""
"Data generation success for this task is typically 65-82% over 1000 "
"demonstrations, taking 18-40 minutes depending on GPU hardware and "
"success rate (18 minutes on a RTX ADA 6000 @ 82% success rate)."
msgstr ""
"此任务的数据生成成功率通常为 65-82%（1000 个演示），需要 18-40 分钟，具体取决于 GPU 硬件和成功率（在 RTX ADA "
"6000 上成功率 82% 时为 18 分钟）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:694
msgid ""
"Behavior Cloning (BC) policy success is typically 75-85% (evaluated on 50"
" rollouts) when trained on 1000 generated demonstrations for 2000 epochs "
"(default), depending on demonstration quality. Training takes "
"approximately 40 minutes on a RTX ADA 6000."
msgstr ""
"行为克隆（BC）策略成功率通常为 75-85%（在 50 次展示中评估），基于 1000 个生成的演示训练 2000 "
"个轮次（默认），具体取决于演示质量。在 RTX ADA 6000 上训练大约需要 40 分钟。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:698
msgid "Generate the dataset with manipulation and point-to-point navigation"
msgstr "生成包含操作和点对点导航的数据集"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:700
msgid ""
"To create a comprehensive locomanipulation dataset that combines both "
"manipulation and navigation capabilities, you can generate a navigation "
"dataset using the manipulation dataset from the previous step as input."
msgstr "要创建结合操作和导航能力的综合运动操作数据集，您可以使用上一步的操作数据集作为输入生成导航数据集。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:702
msgid "G1 humanoid robot combining navigation with locomanipulation"
msgstr "G1 人形机器人结合导航和运动操作"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:708
msgid ""
"G1 humanoid robot performing locomanipulation with navigation "
"capabilities."
msgstr "G1 人形机器人执行具有导航能力的运动操作。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:710
msgid ""
"The locomanipulation dataset generation process takes the previously "
"generated manipulation dataset and creates scenarios where the robot must"
" navigate from one location to another while performing manipulation "
"tasks. This creates a more complex dataset that includes both locomotion "
"and manipulation behaviors."
msgstr "运动操作数据集生成过程采用先前生成的操作数据集，并创建机器人必须在执行操作任务的同时从一个位置导航到另一个位置的场景。这创建了一个更复杂的数据集，包括运动和操作行为。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:712
msgid "To generate the locomanipulation dataset, use the following command:"
msgstr "要生成运动操作数据集，请使用以下命令:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:731
msgid ""
"The input dataset (``--dataset``) should be the manipulation dataset "
"generated in the previous step. You can specify any output filename using"
" the ``--output_file_name`` parameter."
msgstr ""
"输入数据集（ ``--dataset`` ）应该是上一步生成的操作数据集。您可以使用 ``--output_file_name`` "
"参数指定任何输出文件名。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:733
msgid "The key parameters for locomanipulation dataset generation are:"
msgstr "运动操作数据集生成的关键参数是:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:735
msgid ""
"``--lift_step 70``: Number of steps for the lifting phase of the "
"manipulation task.  This should mark the point immediately after the "
"robot has grasped the object."
msgstr "``--lift_step 70``: 操作任务提升阶段的步数。这应该标记机器人抓取物体后的时间点。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:736
msgid ""
"``--navigate_step 120``: Number of steps for the navigation phase between"
" locations.  This should make the point where the robot has lifted the "
"object and is ready to walk."
msgstr "``--navigate_step 120``: 位置之间导航阶段的步数。这应该标记机器人已提起物体并准备行走的时间点。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:737
msgid "``--output_file``: Name of the output dataset file"
msgstr "``--output_file``: 输出数据集文件的名称"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:739
msgid ""
"This process creates a dataset where the robot performs the manipulation "
"task at different locations, requiring it to navigate between points "
"while maintaining the learned manipulation behaviors. The resulting "
"dataset can be used to train policies that combine both locomotion and "
"manipulation capabilities."
msgstr "此过程创建一个数据集，其中机器人在不同位置执行操作任务，需要在保持已学习的操作行为的同时在点之间导航。生成的数据集可用于训练结合运动和操作能力的策略。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:743
msgid ""
"You can visualize the robot trajectory results with the following script "
"command:"
msgstr "您可以使用以下脚本命令可视化机器人轨迹结果:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:749
msgid ""
"The data generated from this locomanipulation pipeline can also be used "
"to finetune an imitation learning policy using GR00T N1.5.  To do this, "
"you may convert the generated dataset to LeRobot format as expected by "
"GR00T N1.5, and then run the finetuning script provided in the GR00T N1.5"
" repository.  An example closed-loop policy rollout is shown in the video"
" below:"
msgstr ""
"从此运动操作管道生成的数据也可用于使用 GR00T N1.5 微调模仿学习策略。为此，您可以将生成的数据集转换为 GR00T N1.5 所需的 "
"LeRobot 格式，然后运行 GR00T N1.5 存储库中提供的微调脚本。下面的视频中显示了一个闭环策略展示示例:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:753
msgid "Simulation rollout of GR00T N1.5 policy finetuned for locomanipulation"
msgstr "针对运动操作微调的 GR00T N1.5 策略的模拟展示"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:759
msgid "Simulation rollout of GR00T N1.5 policy finetuned for locomanipulation."
msgstr "针对运动操作微调的 GR00T N1.5 策略的模拟展示。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:761
msgid ""
"The policy shown above uses the camera image, hand poses, hand joint "
"positions, object pose, and base goal pose as inputs. The output of the "
"model is the target base velocity, hand poses, and hand joint positions "
"for the next several timesteps."
msgstr "上面显示的策略使用相机图像、手部姿态、手部关节位置、物体姿态和基础目标姿态作为输入。模型的输出是接下来几个时间步的目标基础速度、手部姿态和手部关节位置。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:766
msgid "Demo 3: Visuomotor Policy for a Humanoid Robot"
msgstr "演示 3: 人形机器人的视觉运动策略"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:768
#: ../../source/overview/imitation-learning/teleop_imitation.rst:894
msgid "GR-1 humanoid robot performing a pouring task"
msgstr "GR-1 人形机器人执行倾倒任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:775
msgid "Download the Dataset"
msgstr "下载数据集"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:777
msgid ""
"Download the pre-generated dataset from `here "
"<https://download.isaacsim.omniverse.nvidia.com/isaaclab/dataset/generated_dataset_gr1_nut_pouring.hdf5>`__"
" and place it under "
"``IsaacLab/datasets/generated_dataset_gr1_nut_pouring.hdf5`` (**Note: The"
" dataset size is approximately 12GB**). The dataset contains 1000 "
"demonstrations of a humanoid robot performing a pouring/placing task that"
" was generated using Isaac Lab Mimic for the ``Isaac-NutPour-GR1T2-Pink-"
"IK-Abs-Mimic-v0`` task."
msgstr ""
"从 `这里 "
"<https://download.isaacsim.omniverse.nvidia.com/isaaclab/dataset/generated_dataset_gr1_nut_pouring.hdf5>`_"
" 下载预生成的数据集，并将其放置在 "
"``IsaacLab/datasets/generated_dataset_gr1_nut_pouring.hdf5`` 下 (**注意: "
"数据集大小约为 12GB**)。该数据集包含使用Isaac Lab Mimic为 ``Isaac-NutPour-GR1T2-Pink-IK-"
"Abs-Mimic-v0`` 任务生成的一个人形机器人执行倾倒/放置任务的1000次演示。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:783
msgid ""
"If desired, data collection, annotation, and generation can be done using"
" the same commands as the prior examples."
msgstr "如果需要的话，可以使用与之前示例相同的命令进行数据收集、标注和生成。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:785
msgid ""
"The robot first picks up the red beaker and pours the contents into the "
"yellow bowl. Then, it drops the red beaker into the blue bin. Lastly, it "
"places the yellow bowl onto the white scale. See the video in the :ref"
":`visualize-results-demo-2` section below for a visual demonstration of "
"the task."
msgstr ""
"机器人首先拿起红色烧杯，将内容物倒入黄色碗中。然后，它将红色烧杯放入蓝色垃圾箱中。最后，它将黄色碗放在白色天平上。请参见下面 :ref"
":`visualize-results-demo-2` 部分的视频，以获取任务的视觉演示。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:789
msgid ""
"**The success criteria for this task requires the red beaker to be placed"
" in the blue bin, the green nut to be in the yellow bowl, and the yellow "
"bowl to be placed on top of the white scale.**"
msgstr "**此任务的成功标准要求将红色烧杯放置在蓝色垃圾桶中，将绿色螺母放在黄色碗中，并将黄色碗放在白色天平上。**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:793
msgid ""
"**The following commands are only for your reference and are not required"
" for this demo.**"
msgstr "**注意，以下命令仅供参考，对本演示没有必要。**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:806
msgid ""
"Since this is a visuomotor environment, the ``--enable_cameras`` flag "
"must be added to the annotation and data generation commands."
msgstr "由于这是一个视觉运动环境，必须将 ``--enable_cameras`` 标志添加到标注和数据生成命令中。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:821
msgid ""
"There are multiple right eef annotations for this task. Annotations for "
"subtasks for the same eef cannot have the same action index. Make sure to"
" annotate the right eef subtasks with different action indices."
msgstr "有多个正确的该任务的eef标注。相同eef的子任务标注不能具有相同的动作索引。确保用不同的动作索引标注正确的eef子任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:825
msgid "To generate the dataset:"
msgstr "生成数据集: "

#: ../../source/overview/imitation-learning/teleop_imitation.rst:845
msgid ""
"Use `Robomimic <https://robomimic.github.io/>`__ to train a visuomotor BC"
" agent for the task."
msgstr "使用 `Robomimic <https://robomimic.github.io/>`__ 来为任务训练视觉动作 BC 智能体。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:859
msgid ""
"By default the trained models and logs will be saved to "
"``IsaacLab/logs/robomimic``."
msgstr "默认情况下，训练的模型和日志将保存到 ``IssacLab/logs/robomimic`` 中。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:861
msgid ""
"You can also post-train a `GR00T <https://github.com/NVIDIA/Isaac-"
"GR00T>`__ foundation model to deploy a Vision-Language-Action policy for "
"the task."
msgstr ""
"您还可以后训练一个 `GR00T <https://github.com/NVIDIA/Isaac-GR00T>`__ "
"基础模型，为任务部署视觉-语言-动作策略。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:863
msgid ""
"Please refer to the `IsaacLabEvalTasks <https://github.com/isaac-"
"sim/IsaacLabEvalTasks/>`__ repository for more details."
msgstr ""
"有关更多详细信息，请参阅 `IsaacLabEvalTasks <https://github.com/isaac-"
"sim/IsaacLabEvalTasks/>`__ 存储库。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:891
msgid ""
"**If you don't see expected performance results:** Test policies from "
"various checkpoint epochs, not just the final one. Policy performance can"
" vary substantially across training, and intermediate checkpoints often "
"yield better results."
msgstr ""
"**如果您没有看到预期的性能结果：** 请测试来自不同检查点轮次的策略，而不仅仅是最后一个。"
"策略性能在整个训练过程中可能存在显著差异，中间检查点往往会产生更好的结果。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:900
msgid "The trained visuomotor policy performing the pouring task in Isaac Lab."
msgstr "经过训练的视觉运动策略在Isaac Lab中执行倾倒任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:904
msgid "**Expected Success Rates and Timings for Visuomotor Nut Pour GR1T2 Task**"
msgstr "**视觉运动螺母倾倒 GR1T2 任务的预期成功率和时间**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:907
msgid ""
"Data generation for 1000 demonstrations takes approximately 10 hours on a"
" RTX ADA 6000."
msgstr "在 RTX ADA 6000 上生成 1000 个演示大约需要 10 小时。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:908
msgid ""
"Behavior Cloning (BC) policy success is typically 50-60% (evaluated on 50"
" rollouts) when trained on 1000 generated demonstrations for 600 epochs "
"(default). Training takes approximately 15 hours on a RTX ADA 6000."
msgstr ""
"行为克隆（BC）策略成功率通常为 50-60%（在 50 次展示中评估），基于 1000 个生成的演示训练 600 个轮次（默认）。在 RTX "
"ADA 6000 上训练大约需要 15 小时。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:909
msgid ""
"**Recommendation:** Train for 600 epochs with 1000 generated "
"demonstrations, and **evaluate multiple checkpoints saved between the "
"300th and 600th epochs** to select the best-performing policy. Testing "
"various epochs is critical for achieving optimal performance."
msgstr ""
"**建议：** 使用 1000 个生成的演示训练 600 个轮次，并 **评估保存在第 300 到第 600 "
"个轮次之间的多个检查点**，以选择性能最佳的策略。测试不同轮次对于实现最优性能至关重要。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:914
msgid "Common Pitfalls when Generating Data"
msgstr "生成数据时常见的陷阱"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:916
msgid "**Demonstrations are too long:**"
msgstr "**演示时间太长:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:918
msgid "Longer time horizon is harder to learn for a policy"
msgstr "更长的时间范围对于策略来说更难学习"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:919
msgid "Start close to the first object and minimize motions"
msgstr "从第一个物体开始，尽量减少运动"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:921
msgid "**Demonstrations are not smooth:**"
msgstr "**演示不流畅:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:923
msgid "Irregular motion is hard for policy to decipher"
msgstr "不规则运动很难被策略解读"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:924
msgid ""
"Better teleop devices result in better data (i.e. SpaceMouse is better "
"than Keyboard)"
msgstr "更好的遥控设备带来更好的数据（即 SpaceMouse 比 Keyboard 更好）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:926
msgid "**Pauses in demonstrations:**"
msgstr "**演示中的暂停:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:928
msgid "Pauses are difficult to learn"
msgstr "暂停是难以学习的"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:929
msgid "Keep the human motions smooth and fluid"
msgstr "保持人类动作平滑流畅"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:931
msgid "**Excessive number of subtasks:**"
msgstr "**过多的子任务:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:933
msgid "Minimize the number of defined subtasks for completing a given task"
msgstr "最小化完成给定任务所需定义的子任务数量"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:934
msgid ""
"Less subtacks results in less stitching of trajectories, yielding higher "
"data generation success rate"
msgstr "较少的子任务导致较少的轨迹拼接，从而提高了数据生成成功率"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:936
msgid "**Lack of action noise:**"
msgstr "**缺乏动作噪声:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:938
msgid "Action noise makes policies more robust"
msgstr "动作从使策略更加鲁棒"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:940
msgid "**Recording cropped too tight:**"
msgstr "**录制裁剪过于紧密:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:942
msgid ""
"If recording stops on the frame the success term triggers, it may not re-"
"trigger during replay"
msgstr "如果录制在成功项触发的帧上停止，则在回放期间可能不会重新触发"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:943
msgid "Allow for some buffer at the end of recording"
msgstr "允许在录制结束时留出一些缓冲区"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:945
msgid "**Non-deterministic replay:**"
msgstr "**非确定性重放:**"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:947
msgid ""
"Physics in IsaacLab are not deterministically reproducible when using "
"``env.reset`` so demonstrations may fail on replay"
msgstr "在 IsaacLab 中，使用 ``env.reset`` 时，物理仿真无法确定性地复现，因此在回放时，演示可能会失败。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:948
msgid ""
"Collect more human demos than needed, use the ones that succeed during "
"annotation"
msgstr "收集比所需更多的人类演示，使用在标注过程中成功的那些"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:949
msgid ""
"All data in Isaac Lab Mimic generated HDF5 file represent a successful "
"demo and can be used for training (even if non-determinism causes failure"
" when replayed)"
msgstr "所有在 Isaac Lab Mimic 生成的 HDF5 文件中的数据都代表一个成功的演示，并且可以用于训练（即使非确定性在重放时导致失败）。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:953
msgid "Creating Your Own Isaac Lab Mimic Compatible Environments"
msgstr "创建您自己的 Isaac Lab Mimic 兼容环境"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:956
msgid "How it works"
msgstr "它是如何工作的"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:958
msgid ""
"Isaac Lab Mimic works by splitting the input demonstrations into "
"subtasks. Subtasks are user-defined segments in the demonstrations that "
"are common to all demonstrations. Examples for subtasks are \"grasp an "
"object\", \"move end effector to some pre-defined position\", \"release "
"object\" etc.. Note that most subtasks are defined with respect to some "
"object that the robot interacts with."
msgstr ""
"Isaac Lab Mimic 通过将输入的示范分割成子任务来工作。子任务是示范中用户定义的、在所有示范中共有的片段。子任务的例子包括 "
"\"抓取物体\" 、 \"将末端执行器移动到某个预定义位置\" 、 \"释放物体\" "
"等等。请注意，大多数子任务是相对于机器人与之交互的某个物体来定义的。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:960
msgid ""
"Subtasks need to be defined, and then annotated for each input "
"demonstration. Annotation can either happen algorithmically by defining "
"heuristics for subtask detection, as was done in the example above, or it"
" can be done manually."
msgstr "子任务需要被定义，然后为每个输入示例添加标注。标注可以通过定义用于子任务检测的启发式算法来实现，如上面示例中所做的那样，或者也可以手动完成。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:962
msgid ""
"With subtasks defined and annotated, Isaac Lab Mimic utilizes a small "
"number of helper methods to then transform the subtask segments, and "
"generate new demonstrations by stitching them together to match the new "
"task at hand."
msgstr "定义并标注了子任务后，Isaac Lab Mimic 利用少量辅助方法来转换子任务片段，并通过将它们拼接在一起生成新的演示，以匹配当前的任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:964
#, python-format
msgid ""
"For each thusly generated candidate demonstration, Isaac Lab Mimic uses a"
" boolean success criteria to determine whether the demonstration "
"succeeded in performing the task, and if so, add it to the output "
"dataset. Success rate of candidate demonstrations can be as high as 70% "
"in simple cases, and as low as <1%, depending on the difficulty of the "
"task, and the complexity of the robot itself."
msgstr ""
"对于每个这样生成的候选演示，Isaac Lab Mimic "
"使用布尔成功标准来判断演示是否成功执行任务，如果成功，则将其添加到输出数据集。候选演示的成功率在简单情况下可以高达 "
"70%，而在复杂任务和机器人本身的复杂性影响下，成功率可能低于 1%。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:967
msgid "Configuration and subtask definition"
msgstr "配置和子任务定义"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:969
msgid ""
"Subtasks, among other configuration settings for Isaac Lab Mimic, are "
"defined in a Mimic compatible environment configuration class that is "
"created by extending the existing environment config with additional "
"Mimic required parameters."
msgstr ""
"子任务，除了 Isaac Lab Mimic 的其他配置设置外，还在 Mimic 兼容的环境配置类中定义，该类通过扩展现有的环境配置并加入额外的 "
"Mimic 所需参数来创建。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:971
msgid ""
"All Mimic required config parameters are specified in the "
":class:`~isaaclab.envs.MimicEnvCfg` class."
msgstr "所有 Mimic 所需的配置参数都在 :class:`~isaaclab.envs.MimicEnvCfg` 类中指定。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:973
msgid ""
"The config class "
":class:`~isaaclab_mimic.envs.FrankaCubeStackIKRelMimicEnvCfg` serves as "
"an example of creating a Mimic compatible environment config class for "
"the Franka stacking task that was used in the examples above."
msgstr ""
"配置类 :class:`~isaaclab_mimic.envs.FrankaCubeStackIKRelMimicEnvCfg` "
"作为一个示例，用于创建一个与 Mimic 兼容的环境配置类，适用于上面示例中使用的 Franka 堆叠任务。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:975
msgid ""
"The ``DataGenConfig`` member contains various parameters that influence "
"how data is generated. It is initially sufficient to just set the "
"``name`` parameter, and revise the rest later."
msgstr "``DataGenConfig`` 成员包含各种影响数据生成的参数。最初只需要设置 ``name`` 参数，其他的可以稍后修改。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:977
msgid ""
"Subtasks are a list of :class:`~isaaclab.envs.SubTaskConfig` objects, of "
"which the most important members are:"
msgstr "子任务是一个 :class:`~isaaclab.envs.SubTaskConfig` 对象的列表，其中最重要的成员是:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:979
msgid ""
"``object_ref`` is the object that is being interacted with. This will be "
"used to adjust motions relative to this object during data generation. "
"Can be ``None`` if the current subtask does not involve any object."
msgstr ""
"``object_ref`` 是正在交互的对象。它将用于在数据生成过程中相对于该对象调整动作。如果当前子任务不涉及任何对象，则可以是 "
"``None`` 。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:980
msgid ""
"``subtask_term_signal`` is the ID of the signal indicating whether the "
"subtask is active or not."
msgstr "``subtask_term_signal`` 是指示子任务是否处于活动状态的信号的 ID。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:982
msgid ""
"For multi end-effector environments, subtask ordering between end-"
"effectors can be enforced by specifying subtask constraints. These "
"constraints are defined in the "
":class:`~isaaclab.envs.SubTaskConstraintConfig` class."
msgstr ""
"对于具有多个末端执行器的环境，可以通过指定子任务约束来强制定义末端执行器之间的子任务顺序。这些约束在 "
":class:`~isaaclab.envs.SubTaskConstraintConfig` 类中定义。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:985
msgid "Subtask annotation"
msgstr "子任务标注"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:987
msgid ""
"Once the subtasks are defined, they need to be annotated in the source "
"data. There are two methods to annotate source demonstrations for subtask"
" boundaries: Manual annotation or using heuristics."
msgstr "一旦子任务被定义，它们需要在源数据中进行标注。有两种方法可以标注源示范的子任务边界: 手动标注或使用启发式方法。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:989
msgid ""
"It is often easiest to perform manual annotations, since the number of "
"input demonstrations is usually very small. To perform manual "
"annotations, use the ``annotate_demos.py`` script without the ``--auto`` "
"flag. Then press ``B`` to pause, ``N`` to continue, and ``S`` to annotate"
" a subtask boundary."
msgstr ""
"通常手动标注是最简单的，因为输入示例的数量通常非常少。要执行手动标注，请使用 ``annotate_demos.py`` 脚本，且不带 "
"``--auto`` 标志。然后按 ``B`` 暂停，按 ``N`` 继续，按 ``S`` 标注子任务边界。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:991
msgid ""
"For more accurate boundaries, or to speed up repeated processing of a "
"given task for experiments, heuristics can be implemented to perform the "
"same task. Heuristics are observations in the environment. An example how"
" to add subtask terms can be found in "
"``source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/stack/stack_env_cfg.py``,"
" where they are added as an observation group called ``SubtaskCfg``. This"
" example is using prebuilt heuristics, but custom heuristics are easily "
"implemented."
msgstr ""
"为了更精确的边界，或为了加速对给定任务的重复处理以进行实验，可以实现启发式方法来执行相同的任务。启发式方法是对环境中的观测。如何添加子任务项的示例可以在"
" "
"``source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/stack/stack_env_cfg.py``"
" 中找到，其中它们作为一个观测组被添加，名为 ``SubtaskCfg`` 。这个示例使用了预构建的启发式方法，但自定义启发式方法也很容易实现。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:995
msgid "Helpers for demonstration generation"
msgstr "生成演示的帮助程序"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:997
msgid ""
"Helpers needed for Isaac Lab Mimic are defined in the environment. All "
"tasks that are to be used with Isaac Lab Mimic are derived from the "
":class:`~isaaclab.envs.ManagerBasedRLMimicEnv` base class, and must "
"implement the following functions:"
msgstr ""
"在 Isaac Lab Mimic 中需要的帮助程序是在环境中定义的。所有要与 Isaac Lab Mimic 一起使用的任务都源自 "
":class:`~isaaclab.envs.ManagerBasedRLMimicEnv` 基类，并且必须实现以下功能:"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:999
msgid ""
"``get_robot_eef_pose``: Returns the current robot end effector pose in "
"the same frame as used by the robot end effector controller."
msgstr "``get_robot_eef_pose``: 返回当前机器人末端执行器姿态，该姿态与机器人末端执行器控制器使用的坐标系相同。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1001
msgid ""
"``target_eef_pose_to_action``: Takes a target pose and a gripper action "
"for the end effector controller and returns an action which achieves the "
"target pose."
msgstr "``target_eef_pose_to_action``: 获取目标姿态和夹爪动作，供末端执行器控制器使用，并返回一个能够实现目标姿态的动作。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1003
msgid ""
"``action_to_target_eef_pose``: Takes an action and returns a target pose "
"for the end effector controller."
msgstr "``action_to_target_eef_pose``: 采取一个动作并返回末端执行器控制器的目标姿态。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1005
msgid ""
"``actions_to_gripper_actions``: Takes a sequence of actions and returns "
"the gripper actuation part of the actions."
msgstr "``actions_to_gripper_actions``: 接收一系列动作，并返回动作中的夹爪执行器部分。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1007
msgid ""
"``get_object_poses``: Returns the pose of each object in the scene that "
"is used for data generation."
msgstr "``get_object_poses``: 返回场景中用于数据生成的每个对象的姿势。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1009
msgid ""
"``get_subtask_term_signals``: Returns a dictionary of binary flags for "
"each subtask in a task. The flag of true is set when the subtask has been"
" completed and false otherwise."
msgstr ""
"``get_subtask_term_signals``: 返回一个字典，包含任务中每个子任务的二进制标志。当子任务已完成时，标志为 "
"true，否则为 false。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1011
msgid ""
"The class :class:`~isaaclab_mimic.envs.FrankaCubeStackIKRelMimicEnv` "
"shows an example of creating a Mimic compatible environment from an "
"existing Isaac Lab environment."
msgstr ""
"该类 :class:`~isaaclab_mimic.envs.FrankaCubeStackIKRelMimicEnv` 展示了如何从现有的 "
"Isaac Lab 环境创建一个 Mimic 兼容的环境。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1014
msgid "Registering the environment"
msgstr "注册环境"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1016
msgid ""
"Once both Mimic compatible environment and environment config classes "
"have been created, a new Mimic compatible environment can be registered "
"using ``gym.register``. For the Franka stacking task in the examples "
"above, the Mimic environment is registered as ``Isaac-Stack-Cube-Franka-"
"IK-Rel-Mimic-v0``."
msgstr ""
"一旦创建了兼容 Mimic 的环境和环境配置类，就可以使用 ``gym.register`` 注册一个新的 Mimic 兼容环境。对于上面示例中的"
" Franka 堆叠任务，Mimic 环境被注册为 ``Isaac-Stack-Cube-Franka-IK-Rel-Mimic-v0`` 。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1018
msgid "The registered environment is now ready to be used with Isaac Lab Mimic."
msgstr "注册的环境现在已准备好与 Isaac Lab Mimic 一起使用。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1022
msgid "Tips for Successful Data Generation with Isaac Lab Mimic"
msgstr "成功使用 Isaac Lab Mimic 进行数据生成的技巧"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1025
msgid "Splitting subtasks"
msgstr "划分子任务"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1027
msgid ""
"A general rule of thumb is to split the task into as few subtasks as "
"possible, while still being able to complete the task. Isaac Lab Mimic "
"data generation uses linear interpolation to bridge and stitch together "
"subtask segments. More subtasks result in more stitching of trajectories "
"which can result in less smooth motions and more failed demonstrations. "
"For this reason, it is often best to annoatate subtask boundaries where "
"the robot's motion is unlikely to collide with other objects."
msgstr ""
"一个基本的经验法则是把任务分解为尽可能少的子任务，同时仍能完成任务。Isaac Lab "
"Mimic数据生成使用线性插值来连接和拼接子任务段。更多的子任务会导致轨迹的拼接更多，这可能导致动作更不平滑，演示失败的可能性更大。因此，通常最好在机器人的运动不太可能与其他物体相撞的地方标记子任务边界。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1030
msgid ""
"For example, in the scenario below, there is a subtask partition after "
"the robot's left arm grasps the object. On the left, the subtask "
"annotation is marked immediately after the grasp, while on the right, the"
" annotation is marked after the robot has grasped and lifted the object. "
"In the left case, the interpolation causes the robot's left arm to "
"collide with the table and it's motion lags while on the right the motion"
" is continuous and smooth."
msgstr "例如，在下面的情况中，机器人的左臂抓取物体后有一个子任务分区。在左侧，子任务标注在抓取后立即标记，而在右侧，在机器人抓取并提起物体后标记标注。在左侧情况下，插值导致机器人的左臂与桌子碰撞，并且它的运动延迟，而在右侧，运动是连续和平滑的。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1033
msgid "Subtask splitting example"
msgstr "子任务拆分示例"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1041
msgid "Motion lag/collision caused by poor subtask splitting (left)"
msgstr "由于不良子任务分割造成的运动延迟/碰撞（向左）"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1043
msgid "Selecting number of interpolation steps"
msgstr "选择插值步数"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1045
msgid ""
"The number of interpolation steps between subtask segments can be "
"specified in the :class:`~isaaclab.envs.SubTaskConfig` class. Once "
"transformed, the subtask segments don't start/end at the same spot, thus "
"to create a continuous motion, Isaac Lab Mimic will apply linear "
"interpolation between the last point of the previous subtask and the "
"first point of the next subtask."
msgstr ""
"在 :class:`~isaaclab.envs.SubTaskConfig` "
"类中可以指定子任务段之间的插值步数。转换后，子任务段不会在同一位置开始/结束，因此为了创建连续运动，Isaac Lab Mimic "
"将在前一个子任务的最后一个点和下一个子任务的第一个点之间应用线性插值。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1048
msgid ""
"The number of interpolation steps can be tuned to control the smoothness "
"of the generated demonstrations during this stitching process. The "
"appropriate number of interpolation steps depends on the speed of the "
"robot and the complexity of the task. A complex task with a large object "
"reset distribution will have larger gaps between subtask segments and "
"require more interpolation steps to create a smooth motion. "
"Alternatively, a task with small gaps between subtask segments should use"
" a small number of interpolation steps to avoid unnecessary motion lag "
"caused by too many steps."
msgstr "插值步数的数量可以调整，以控制在这个拼接过程中生成的演示的平滑度。适当的插值步数取决于机器人的速度和任务的复杂性。具有较大物体重置分布的复杂任务将在子任务段之间具有更大的间隙，并需要更多的插值步骤来创建平滑运动。另外，对于子任务段之间间隔较小的任务，应该使用较少的插值步骤，以避免由于过多步骤导致的不必要的动作延迟。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1052
msgid ""
"An example of how the number of interpolation steps can affect the "
"generated demonstrations is shown below. In the example, an interpolation"
" is applied to the right arm of the robot to bridge the gap between the "
"left arm's grasp and the right arm's placement. With 0 steps, the right "
"arm exhibits a jerky jump in motion while with 20 steps, the motion is "
"laggy. With 5 steps, the motion is smooth and natural."
msgstr "以下显示了插值步数如何影响生成的演示示例。在这个示例中，将对机器人的右臂进行插值，以弥合左臂抓取和右臂放置之间的差距。在0步的情况下，右臂在运动中出现突然跳跃，而在20步的情况下，运动会显得延迟。在5步的情况下，运动是平滑而自然的。"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1068
msgid "|0_interp_steps| |5_interp_steps| |20_interp_steps|"
msgstr "|0_interp_steps| |5_interp_steps| |20_interp_steps|"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1056
msgid "GR-1 robot with 0 interpolation steps"
msgstr "GR-1 机器人，0 插值步骤"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1060
msgid "GR-1 robot with 5 interpolation steps"
msgstr "GR-1 机器人，5 个插补步骤"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1064
msgid "GR-1 robot with 20 interpolation steps"
msgstr "GR-1 机器人，20 个插值步骤"

#: ../../source/overview/imitation-learning/teleop_imitation.rst:1070
msgid "Left: 0 steps. Middle: 5 steps. Right: 20 steps."
msgstr "左侧: 0 步。中间: 5 步。右侧: 20 步。"

